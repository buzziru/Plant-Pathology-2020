{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T23:38:26.167744Z",
     "iopub.status.busy": "2026-01-04T23:38:26.167583Z",
     "iopub.status.idle": "2026-01-04T23:38:37.851084Z",
     "shell.execute_reply": "2026-01-04T23:38:37.849879Z",
     "shell.execute_reply.started": "2026-01-04T23:38:26.167726Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import wandb\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "wandb_api_key = os.getenv(\"WANDB_API_KEY\")\n",
    "\n",
    "wandb.login(key=wandb_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-01-04T23:38:43.012538Z",
     "iopub.status.busy": "2026-01-04T23:38:43.012198Z",
     "iopub.status.idle": "2026-01-04T23:38:57.834656Z",
     "shell.execute_reply": "2026-01-04T23:38:57.833404Z",
     "shell.execute_reply.started": "2026-01-04T23:38:43.012501Z"
    },
    "id": "5qI9d5wIEJiY",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='pydantic')\n",
    "\n",
    "import gc\n",
    "import math\n",
    "import shutil\n",
    "import glob\n",
    "import random\n",
    "from types import SimpleNamespace\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import cv2\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.swa_utils import update_bn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from lightning.pytorch.callbacks import TQDMProgressBar\n",
    "\n",
    "from torchmetrics import ConfusionMatrix, AUROC\n",
    "import timm\n",
    "import ttach as tta\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cv2.setNumThreads(0)\n",
    "print(os.cpu_count())\n",
    "torch.set_float32_matmul_precision('medium') # L4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    model_arch = 'convnext_small.fb_in22k_ft_in1k'\n",
    "    is_bn = False\n",
    "    seed = 855\n",
    "    top_k = 3\n",
    "    n_folds = 5\n",
    "    epochs = 25\n",
    "    virtual_epochs = 25\n",
    "    warmup_multiplier = 2\n",
    "    batch_size = 32\n",
    "    accum_iter = 1\n",
    "    num_workers = 4\n",
    "    persistent_workers=True\n",
    "    lr = 0.0001\n",
    "    weight_decay = 0.05\n",
    "    alpha = 0.5\n",
    "    T = 2.0\n",
    "    drop_path_rate = 0.2\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    project_name = 'PlantPathology2020'\n",
    "    exp_name = 'Student9_convnext_small_inf_edited'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed, deterministic=False):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed) # cpu\n",
    "    torch.cuda.manual_seed(seed) # gpu\n",
    "    torch.cuda.manual_seed_all(seed) # 멀티 gpu\n",
    "    if deterministic:\n",
    "        os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.use_deterministic_algorithms(True)\n",
    "    else:\n",
    "        torch.backends.cudnn.deterministic = False\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        torch.use_deterministic_algorithms(False)\n",
    "\n",
    "\n",
    "set_seed(CFG.seed)\n",
    "\n",
    "device = CFG.device\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(CFG.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dWWDJu8cEJiZ"
   },
   "source": [
    "# 2. Data Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModule:\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "        self.img_dir = self.data_dir + 'images/'\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        self.train_df = pd.read_csv(self.data_dir + 'datasets/train_reborn_02.csv')\n",
    "        self.train_df = self.train_df.reset_index(drop=True)\n",
    "        self.test_df = pd.read_csv(self.data_dir + 'test.csv')\n",
    "        self.submission = pd.read_csv(self.data_dir + 'sample_submission.csv')\n",
    "        \n",
    "        oof_df_01 = pd.read_csv(self.data_dir + 'datasets/oof_preds_Student5_EfficientNetB6_reborn.csv')\n",
    "        oof_df_02 = pd.read_csv(self.data_dir + 'oof_preds_Student8_ResNest101e.csv')        \n",
    "        oof_df_01['image_id'] = self.train_df['image_id']\n",
    "        oof_df_01 = oof_df_01[['image_id', 'healthy', 'multiple_diseases', 'rust', 'scab']]\n",
    "        \n",
    "        self.oof_df = oof_df_01.copy()\n",
    "        hard_cols = ['healthy', 'multiple_diseases', 'rust', 'scab']\n",
    "        self.oof_df[hard_cols] = oof_df_01[hard_cols] * 0.5 + oof_df_02[hard_cols] * 0.5\n",
    "        self.oof_df.columns = ['image_id', 'healthy_pred', 'multiple_diseases_pred', 'rust_pred', 'scab_pred']\n",
    "        self.train_df = self.train_df.merge(self.oof_df, on='image_id', how='left')\n",
    "        return self.train_df, self.test_df, self.submission\n",
    "\n",
    "    \n",
    "train_df, test_df, submission = DataModule('../data/').prepare_data()\n",
    "\n",
    "hard_cols = ['healthy', 'multiple_diseases', 'rust', 'scab']\n",
    "soft_cols = ['healthy_pred', 'multiple_diseases_pred', 'rust_pred', 'scab_pred']\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Load Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images = {}\n",
    "all_img_ids = np.concatenate([train_df['image_id'].tolist(), test_df['image_id'].tolist()])\n",
    "\n",
    "print(\"Loading all images into RAM once...\")\n",
    "for img_id in tqdm(all_img_ids, desc='Loading Images...' ,leave=False):\n",
    "    img = cv2.imread('../data/images/' + img_id + '.jpg')\n",
    "    img = cv2.resize(img, (650, 450))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img.setflags(write=False)\n",
    "    all_images[img_id] = img\n",
    "\n",
    "print(\"All Images on Ram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, df, hard_cols=hard_cols, soft_cols=soft_cols, transform=None, is_test=False):\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        self.is_test = is_test\n",
    "        self.hard_cols = hard_cols\n",
    "        self.soft_cols = soft_cols\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.df.iloc[idx, 0]\n",
    "        image = all_images[img_id].copy()\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image=image)['image']\n",
    "\n",
    "        if self.is_test:\n",
    "            return image\n",
    "        else:\n",
    "            soft_labels = self.df.iloc[idx][self.soft_cols].values.astype(np.float32)\n",
    "            hard_labels = self.df.iloc[idx][self.hard_cols].values.astype(np.float32)\n",
    "            return image, torch.tensor(soft_labels), torch.tensor(hard_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlantDataModule(pl.LightningDataModule):\n",
    "    \"\"\"\n",
    "    데이터 로딩, 전처리 및 학습/검증 세트 분할을 관리하는 클래스입니다.\n",
    "    \n",
    "    K-Fold 인덱스에 따라 데이터를 학습용과 검증용으로 분리하며, \n",
    "    stage 인자에 따라 불필요한 데이터 로딩을 방지하여 메모리 효율성을 최적화합니다. \n",
    "    이미지 증강(Augmentation) 로직을 내부적으로 포함하여 데이터와 모델 사이의 인터페이스를 명확히 정의합니다.\n",
    "    \"\"\"\n",
    "    def __init__(self, train_df, test_df, cfg, fold_idx, inference_mode=False):\n",
    "        super().__init__()\n",
    "        self.train_df = train_df\n",
    "        self.test_df = test_df\n",
    "        self.cfg = cfg\n",
    "        self.fold_idx = fold_idx\n",
    "        self.inference_mode = inference_mode\n",
    "\n",
    "        self.transform_train = A.Compose([\n",
    "            A.OneOf([\n",
    "                A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0, p=1.0),\n",
    "                A.RandomBrightnessContrast(brightness_limit=0, contrast_limit=0.2, p=1.0)\n",
    "            ], p=1.0),\n",
    "\n",
    "            A.OneOf([\n",
    "                A.MotionBlur(blur_limit=3, p=1.0),\n",
    "                A.MedianBlur(blur_limit=3, p=1.0),\n",
    "                A.GaussianBlur(blur_limit=3, p=1.0),\n",
    "            ], p=0.5),\n",
    "\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            \n",
    "            A.Affine(\n",
    "                scale=(0.8, 1.2),\n",
    "                translate_percent=0.2,\n",
    "                rotate=20,\n",
    "                interpolation=cv2.INTER_CUBIC, # 보간\n",
    "                border_mode=cv2.BORDER_REFLECT_101, # 테두리 반사 채우기\n",
    "                p=1.0\n",
    "            ),\n",
    "\n",
    "            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "\n",
    "        self.transform_test = A.Compose([\n",
    "            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "            \n",
    "    def setup(self, stage=None):\n",
    "        if stage == 'fit' or stage is None:\n",
    "            self.train = self.train_df[self.train_df['fold']!=self.fold_idx].reset_index(drop=True).copy()\n",
    "            self.valid = self.train_df[self.train_df['fold']==self.fold_idx].copy()\n",
    "            self.dataset_train = ImageDataset(self.train, transform=self.transform_train)\n",
    "            self.dataset_valid = ImageDataset(self.valid, transform=self.transform_test)\n",
    "            print(f'[Fit] Train: {len(self.train)}, Valid: {len(self.valid)}')\n",
    "\n",
    "        elif stage == 'test':\n",
    "            self.valid = self.train_df[self.train_df['fold']==self.fold_idx].copy()\n",
    "            self.dataset_valid = ImageDataset(self.valid, transform=self.transform_test)\n",
    "            self.dataset_test = ImageDataset(self.test_df, transform=self.transform_test, is_test=True)\n",
    "            print(f'[Test] Valid(OOF): {len(self.valid)}, Test: {len(self.test_df)}')\n",
    "\n",
    "        elif stage == 'predict':\n",
    "            self.dataset_test = ImageDataset(self.test_df, transform=self.transform_test, is_test=True)\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        loader_train = DataLoader(self.dataset_train, batch_size=self.cfg.batch_size, shuffle=True,\n",
    "                                worker_init_fn=seed_worker, generator=g, num_workers=self.cfg.num_workers, \n",
    "                                persistent_workers=True, pin_memory=True)\n",
    "        return loader_train\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        user_persistent = not self.inference_mode\n",
    "        loader_valid = DataLoader(self.dataset_valid, batch_size=self.cfg.batch_size*4, shuffle=False,\n",
    "                                worker_init_fn=seed_worker, generator=g, num_workers=self.cfg.num_workers, \n",
    "                                persistent_workers=user_persistent, pin_memory=True)\n",
    "        return loader_valid\n",
    "    \n",
    "    def predict_dataloader(self):\n",
    "        loader_test = DataLoader(self.dataset_test, batch_size=self.cfg.batch_size*4, shuffle=False,\n",
    "                                worker_init_fn=seed_worker, generator=g, num_workers=self.cfg.num_workers, \n",
    "                                persistent_workers=False, pin_memory=True)\n",
    "        return loader_test\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return self.predict_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oohzJHhyEJic"
   },
   "source": [
    "# 3. Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlantDiseaseModule(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    모델의 순전파, 손실 함수 계산, 최적화 알고리즘 및 메트릭 측정을 캡슐화합니다.\n",
    "    특히 훈련 단계에서는 Soft Label Mixing(Knowledge Distillation 원리 적용)을 통해 \n",
    "    라벨 노이즈에 대한 강건성을 확보하며, 추론 단계에서는 TTA(Test Time Augmentation)를 \n",
    "    통합하여 예측의 불확실성을 줄이고 일반화 성능을 향상시킵니다.\n",
    "    \"\"\"\n",
    "    def __init__(self, config, steps_per_epoch=None):\n",
    "        super().__init__()\n",
    "        if isinstance(config, type):\n",
    "            config = {k: v for k, v in config.__dict__.items() if not k.startswith('__')}\n",
    "        self.save_hyperparameters(config)\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "        self.model = timm.create_model(\n",
    "            self.hparams.model_arch,\n",
    "            pretrained=True,\n",
    "            drop_path_rate=self.hparams.drop_path_rate,\n",
    "            num_classes=4\n",
    "            )\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "        \n",
    "        # TTA\n",
    "        transforms = tta.Compose([\n",
    "                tta.HorizontalFlip(),\n",
    "                tta.VerticalFlip(),\n",
    "            ])\n",
    "        \n",
    "        self.tta_model = tta.ClassificationTTAWrapper(self.model, transforms, merge_mode='mean')\n",
    "        \n",
    "        # metrics\n",
    "        self.valid_auc = AUROC(task='multiclass', num_classes=4)\n",
    "        self.valid_cm = ConfusionMatrix(task='multiclass', num_classes=4)\n",
    "        self.best_score = 0.0\n",
    "\n",
    "        self.top_k_scores = []  # (score, epoch) 튜플을 저장할 리스트\n",
    "        self.top_k = self.hparams.top_k\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=self.hparams.lr, weight_decay=self.hparams.weight_decay)\n",
    "    \n",
    "        total_steps = self.steps_per_epoch * self.hparams.virtual_epochs\n",
    "        warmup_steps = self.steps_per_epoch * self.hparams.warmup_multiplier\n",
    "            \n",
    "        scheduler = get_cosine_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=warmup_steps,\n",
    "            num_training_steps=total_steps\n",
    "        )\n",
    "        \n",
    "        scheduler_config = {\n",
    "            'scheduler' : scheduler,\n",
    "            'interval' : 'step',\n",
    "            'frequency' : 1\n",
    "        }\n",
    "        \n",
    "        return [optimizer], [scheduler_config]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        image, soft_labels, hard_labels = batch\n",
    "        \n",
    "        T = self.hparams.T\n",
    "        if T > 1.0:\n",
    "            epsilon = 1e-6\n",
    "            logits_from_oof = torch.log(soft_labels + epsilon)\n",
    "            soft_labels = torch.softmax(logits_from_oof / T, dim=1)\n",
    "\n",
    "        label = self.hparams.alpha * hard_labels + (1 - self.hparams.alpha) * soft_labels\n",
    "        outputs = self.model(image)\n",
    "        logits = outputs / T if T > 1 else outputs\n",
    "        loss = self.criterion(logits, label)\n",
    "        \n",
    "        if T > 1.0:\n",
    "            loss = loss * (T ** 2)\n",
    "\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        image, _, hard_labels = batch\n",
    "        outputs = self.model(image)\n",
    "        loss = self.criterion(outputs, hard_labels)\n",
    "        \n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        targets = torch.argmax(hard_labels, dim=1)\n",
    "\n",
    "        self.valid_cm(preds, targets)\n",
    "        self.valid_auc(probs, targets)\n",
    "        \n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('val_roc_auc', self.valid_auc, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        if self.trainer.sanity_checking:\n",
    "            return\n",
    "                  \n",
    "        score = self.trainer.callback_metrics.get('val_roc_auc')\n",
    "        train_loss = self.trainer.callback_metrics.get('train_loss')\n",
    "        val_loss = self.trainer.callback_metrics.get('val_loss')\n",
    "\n",
    "        current_epoch = self.current_epoch\n",
    "        t_loss_str = f\"{train_loss:.4f}\" if train_loss is not None else \"N/A\"\n",
    "        v_loss_str = f\"{val_loss:.4f}\" if val_loss is not None else \"N/A\"\n",
    "        roc_str = f\"{score:.4f}\" if score is not None else \"N/A\"\n",
    "        self.print(f\"\\n(Epoch {current_epoch}) Train Loss: {t_loss_str} | Val Loss: {v_loss_str} | ROC AUC: {roc_str}\")\n",
    "        \n",
    "        if score is not None:\n",
    "            current_score = score.item()            \n",
    "            self.top_k_scores.append((current_score, current_epoch))\n",
    "            self.top_k_scores.sort(key=lambda x: x[0], reverse=True)\n",
    "            self.top_k_scores = self.top_k_scores[:self.top_k]\n",
    "            is_in_top_k = (current_score, current_epoch) in self.top_k_scores\n",
    "            \n",
    "            if is_in_top_k and isinstance(self.logger, WandbLogger):\n",
    "                rank = self.top_k_scores.index((current_score, current_epoch)) + 1\n",
    "                self.print(f'New Top-K Score! (Rank {rank})')\n",
    "                top_k_str = \", \".join([f\"(Ep {e}: {s:.4f})\" for s, e in self.top_k_scores])\n",
    "                self.print(f\"Current Top-{self.top_k}: {top_k_str}\")\n",
    "                \n",
    "                plt.figure(figsize=(10, 8))\n",
    "                cm = self.valid_cm.compute().cpu().numpy()\n",
    "                columns = ['Healthy', 'Multiple', 'Rust', 'Scab']\n",
    "\n",
    "                sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                            xticklabels=columns, yticklabels=columns,\n",
    "                            annot_kws={\"size\": 12}) # 글자 크기 키움\n",
    "                            \n",
    "                plt.ylabel('True Label', fontsize=12)\n",
    "                plt.xlabel('Predicted Label', fontsize=12)\n",
    "                plt.title(f'Confusion Matrix (Epoch {current_epoch})', fontsize=14)\n",
    "\n",
    "                log_key = f\"Confusion_Matrix_Ep{current_epoch}\"                \n",
    "                self.logger.experiment.log({\n",
    "                    log_key: wandb.Image(plt),\n",
    "                    \"global_step\": self.global_step\n",
    "                })\n",
    "                plt.close()\n",
    "                self.print(f\"Confusion Matrix saved to WandB key: {log_key}\")\n",
    "        self.valid_cm.reset()\n",
    "    \n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        if isinstance(batch, (list, tuple)):\n",
    "            x = batch[0]\n",
    "        else:\n",
    "            x = batch\n",
    "        \n",
    "        # outputs = self.tta_model(x)\n",
    "        outputs = self.model(x)\n",
    "        preds = torch.softmax(outputs, dim=1)\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Training Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "OdZxEQE7yhZy"
   },
   "outputs": [],
   "source": [
    "class MetricHandler:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.preds_list = []\n",
    "        self.actual_list = []\n",
    "\n",
    "    def update(self, preds, actual):\n",
    "        self.preds_list.extend(preds)\n",
    "        self.actual_list.extend(actual)\n",
    "\n",
    "    def compute_roc_auc(self):\n",
    "        return roc_auc_score(self.actual_list, self.preds_list)\n",
    "    \n",
    "    \n",
    "class BackupHandler:\n",
    "    def __init__(self, local_dir, backup_dir=None, active=True):\n",
    "        self.local_dir = local_dir\n",
    "        self.backup_dir = backup_dir\n",
    "        self.active = active and (backup_dir is not None)\n",
    "\n",
    "        if self.active and self.backup_dir is not None:\n",
    "            os.makedirs(self.backup_dir, exist_ok=True)\n",
    "            print(f'Backup Active : {self.local_dir} -> {self.backup_dir}')\n",
    "\n",
    "    def backup(self, filename):\n",
    "        if not self.active or self.backup_dir is None:\n",
    "            return\n",
    "\n",
    "        src_path = os.path.join(self.local_dir, filename)\n",
    "        dst_path = os.path.join(self.backup_dir, filename)\n",
    "        \n",
    "        if os.path.exists(src_path):\n",
    "            shutil.copy(src_path, dst_path)\n",
    "\n",
    "    def save_csv(self, df, filename):\n",
    "        local_path = os.path.join(self.local_dir, filename)\n",
    "        df.to_csv(local_path, index=False)\n",
    "        print(f'CSV saved at {local_path}')\n",
    "\n",
    "        if self.active and self.backup_dir is not None:\n",
    "            backup_path = os.path.join(self.backup_dir, filename)\n",
    "            df.to_csv(backup_path, index=False)\n",
    "            print(f'CSV saved at {backup_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Experiment Ochestrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentRunner:\n",
    "    \"\"\"\n",
    "    K-Fold 교차 검증 및 전체 실험 프로세스를 지휘하는 오케스트레이터 클래스입니다.\n",
    "    \n",
    "    환경 설정(Kaggle, Colab, Local)에 따른 경로 자동화부터 WandB 로깅, 체크포인트 저장, \n",
    "    K-Fold 학습 루프 제어 및 최종 추론(OOF 및 Test)까지의 전체 워크플로우를 담당합니다.\n",
    "    실험이 종료될 때마다 명시적인 메모리 정리(GC, CUDA Cache)를 수행하여 \n",
    "    리소스 사용을 최적화하고 연속적인 실험 안정성을 보장합니다.\n",
    "    \"\"\"\n",
    "    def __init__(self, config, train_df, test_df):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.train_df = train_df\n",
    "        self.test_df = test_df\n",
    "        self.paths = self._setup_env()\n",
    "        self.backup_handler = BackupHandler(local_dir=self.paths.local_path , backup_dir=self.paths.drive_path, active=False)\n",
    "        \n",
    "    def _setup_env(self):\n",
    "        is_kaggle = os.path.exists('/kaggle/') \n",
    "        is_colab = os.path.exists('/content/drive/Mydrive') and not is_kaggle\n",
    "\n",
    "        if is_kaggle:\n",
    "            print(\"Environment: Kaggle\")\n",
    "            drive_path = None\n",
    "            local_path = '/kaggle/working/'\n",
    "        elif is_colab:\n",
    "            print(\"Environment: Google Colab\")\n",
    "            drive_path = f'/content/drive/MyDrive/Kaggle_Save/{CFG.exp_name}/'\n",
    "            local_path = '/content/models/'\n",
    "        else:\n",
    "            print(\"Environment: Local\")\n",
    "            drive_path = None\n",
    "            local_path = f'../data/models/{CFG.exp_name}/'\n",
    "        \n",
    "        print(f\"Save Path: {local_path}\")\n",
    "        return SimpleNamespace(local_path=local_path, drive_path=drive_path)    \n",
    "    \n",
    "    def run(self):\n",
    "        for fold in range(self.config.n_folds):\n",
    "            print('='*30, f'FOLD {fold+1}', '='*30)\n",
    "            \n",
    "            wandb_logger = WandbLogger(\n",
    "                project=self.config.project_name,\n",
    "                group=self.config.exp_name,\n",
    "                name=f\"Fold_{fold+1}\",\n",
    "                job_type=\"train\",\n",
    "                config={k: v for k, v in self.config.__dict__.items() if not k.startswith('__')}\n",
    "            )\n",
    "            \n",
    "            train_len = len(self.train_df[self.train_df['fold'] != fold])\n",
    "            steps_per_epoch = math.ceil(train_len / self.config.batch_size / self.config.accum_iter)\n",
    "\n",
    "            datamodule = PlantDataModule(train_df=self.train_df, test_df=self.test_df, cfg=self.config, fold_idx=fold)\n",
    "            model = PlantDiseaseModule(self.config, steps_per_epoch=steps_per_epoch)\n",
    "            \n",
    "            ckpt_callback = pl.callbacks.ModelCheckpoint(\n",
    "                monitor='val_roc_auc',\n",
    "                mode='max',\n",
    "                save_top_k=self.config.top_k,\n",
    "                save_last=False,\n",
    "                dirpath=self.paths.local_path,\n",
    "                filename=f'Fold{fold+1}-Ep{{epoch:02d}}-{{val_roc_auc:.4f}}',\n",
    "                auto_insert_metric_name=False,\n",
    "            )\n",
    "            progress_bar = TQDMProgressBar(refresh_rate=1)\n",
    "            \n",
    "            trainer = pl.Trainer(\n",
    "                max_epochs=self.config.epochs,\n",
    "                accelerator='auto',\n",
    "                precision='16-mixed',\n",
    "                accumulate_grad_batches=self.config.accum_iter,\n",
    "                callbacks=[ckpt_callback, progress_bar],\n",
    "                logger=wandb_logger,\n",
    "                log_every_n_steps=10\n",
    "            )\n",
    "\n",
    "            trainer.fit(model, datamodule=datamodule)\n",
    "            \n",
    "            print(f'\\n Top-{ckpt_callback.save_top_k} Models in this Fold:')\n",
    "            for path, score in ckpt_callback.best_k_models.items():\n",
    "                model_name = os.path.basename(path)\n",
    "                print(f'> {model_name}')\n",
    "                \n",
    "            wandb.finish()\n",
    "            \n",
    "            # 메모리 정리\n",
    "            del datamodule, trainer, model\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "    def _load_averaged_model(self, fold):\n",
    "        save_path = os.path.join(self.paths.local_path, f'best_score_model_{fold+1}.pth')\n",
    "        model = PlantDiseaseModule(self.config)\n",
    "\n",
    "        if os.path.exists(save_path):\n",
    "            print(f'Found existing averaged model for Fold {fold+1}. Loading directly...')\n",
    "            state_dict = torch.load(save_path, map_location=device)\n",
    "            model.load_state_dict(state_dict)\n",
    "        else:\n",
    "            print(f'Merging Top-K Models for Fold {fold+1} ...')\n",
    "            score_pattern = os.path.join(self.paths.local_path, f'Fold{fold+1}-Ep*.ckpt')\n",
    "            score_files = glob.glob(score_pattern)\n",
    "            print(f'Found {len(score_files)} score models : {[os.path.basename(f) for f in score_files]}')\n",
    "            \n",
    "            first_state = torch.load(score_files[0], map_location='cpu')['state_dict']\n",
    "            avg_state_dict = {}\n",
    "            for k, v in first_state.items():\n",
    "                if v.is_floating_point():\n",
    "                    avg_state_dict[k] = v.float() # Float32로 변환하여 초기화\n",
    "                else:\n",
    "                    avg_state_dict[k] = v \n",
    "            \n",
    "            if len(score_files) > 1:\n",
    "                for path in score_files[1:]:\n",
    "                    state_dict = torch.load(path, map_location='cpu')['state_dict']\n",
    "                    for key in avg_state_dict:\n",
    "                        avg_state_dict[key] += state_dict[key].float()\n",
    "                for key in avg_state_dict:\n",
    "                    if avg_state_dict[key].is_floating_point():\n",
    "                        avg_state_dict[key] = avg_state_dict[key] / len(score_files)\n",
    "            \n",
    "            model.load_state_dict(avg_state_dict)\n",
    "            \n",
    "            # for remove_path in score_files:\n",
    "            #     if os.path.exists(remove_path):\n",
    "            #         os.remove(remove_path)\n",
    "            \n",
    "            save_path = os.path.join(self.paths.local_path, f'best_score_model_{fold+1}.pth')\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print('Save Avg Model : ', save_path)\n",
    "        \n",
    "        if self.config.is_bn:\n",
    "            print('Update BN stats ... ')\n",
    "            model = model.to(self.config.device)\n",
    "            model.train()\n",
    "\n",
    "            train_subset = self.train_df[self.train_df['fold'] != fold].reset_index(drop=True)\n",
    "            transform_test = A.Compose([\n",
    "                A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "                ToTensorV2()\n",
    "            ])\n",
    "            dataset_bn = ImageDataset(train_subset, transform=transform_test)\n",
    "            loader_bn = DataLoader(\n",
    "                dataset_bn, \n",
    "                batch_size=self.config.batch_size, \n",
    "                shuffle=True,\n",
    "                num_workers=self.config.num_workers,\n",
    "                pin_memory=True\n",
    "            )\n",
    "            \n",
    "            update_bn(loader_bn, model, device=self.config.device)\n",
    "            model.eval()\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "        else:\n",
    "            print('Skipping BN update for LayerNorm')\n",
    "            model = model.to(self.config.device)\n",
    "        return model\n",
    "        \n",
    "    def run_inference(self):\n",
    "        oof_preds = np.zeros((len(self.train_df), 4))\n",
    "        final_preds = np.zeros((len(self.test_df), 4))\n",
    "\n",
    "        for fold in range(self.config.n_folds):\n",
    "            print(f'=== Inference Fold {fold+1} ===')\n",
    "            avg_model = self._load_averaged_model(fold)\n",
    "            \n",
    "            infer_module = PlantDataModule(self.train_df, self.test_df, self.config, fold_idx=fold, inference_mode=True)\n",
    "            infer_module.setup(stage='test')\n",
    "            \n",
    "            progress_bar = TQDMProgressBar(refresh_rate=1)\n",
    "            infer_trainer = pl.Trainer(\n",
    "                accelerator='auto',\n",
    "                logger=False,\n",
    "                enable_checkpointing=False,\n",
    "                callbacks=[progress_bar]\n",
    "            )\n",
    "            \n",
    "            # OOF Inference\n",
    "            print(f'OOF Inference ... ')\n",
    "            oof_list = infer_trainer.predict(avg_model, dataloaders=infer_module.val_dataloader())\n",
    "            oof_temp = torch.cat(oof_list).cpu().numpy()\n",
    "            valid_indices = infer_module.valid.index.values\n",
    "            oof_preds[valid_indices] = oof_temp\n",
    "\n",
    "            # Test Inference\n",
    "            print(f'Test Inference ... ')\n",
    "            sub_list = infer_trainer.predict(avg_model, dataloaders=infer_module.predict_dataloader())\n",
    "            sub_temp = torch.cat(sub_list).cpu().numpy()\n",
    "            final_preds += (sub_temp / self.config.n_folds)\n",
    "\n",
    "            del infer_trainer, infer_module, avg_model\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "        metric_handler = MetricHandler()\n",
    "        metric_handler.update(oof_preds, self.train_df[hard_cols].values)\n",
    "        oof_roc = metric_handler.compute_roc_auc()\n",
    "        print(f'OOF ROC AUC : {oof_roc:.4f}')\n",
    "\n",
    "        return oof_preds, final_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Training Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = ExperimentRunner(config=CFG, train_df=train_df, test_df=test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Inference & Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "oof_preds, final_preds = runner.run_inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_oof = train_df[['image_id']].copy()\n",
    "result_oof[hard_cols] = oof_preds\n",
    "runner.backup_handler.save_csv(result_oof, f'oof_preds_{CFG.exp_name}.csv')\n",
    "\n",
    "result_sub = submission[['image_id']].copy()\n",
    "result_sub[hard_cols] = final_preds\n",
    "runner.backup_handler.save_csv(result_sub, f'submission_{CFG.exp_name}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N1w8ZC3_EJie",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "display(result_oof.head())\n",
    "display(result_sub.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 1026645,
     "sourceId": 18648,
     "sourceType": "competition"
    },
    {
     "datasetId": 9113217,
     "sourceId": 14395474,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31236,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
