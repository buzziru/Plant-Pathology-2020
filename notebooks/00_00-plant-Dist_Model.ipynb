{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T23:38:26.167744Z",
     "iopub.status.busy": "2026-01-04T23:38:26.167583Z",
     "iopub.status.idle": "2026-01-04T23:38:37.851084Z",
     "shell.execute_reply": "2026-01-04T23:38:37.849879Z",
     "shell.execute_reply.started": "2026-01-04T23:38:26.167726Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /teamspace/studios/this_studio/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mparaise\u001b[0m (\u001b[33mparaise-\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import wandb\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "wandb_api_key = os.getenv(\"WANDB_API_KEY\")\n",
    "\n",
    "wandb.login(key=wandb_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-01-04T23:38:43.012538Z",
     "iopub.status.busy": "2026-01-04T23:38:43.012198Z",
     "iopub.status.idle": "2026-01-04T23:38:57.834656Z",
     "shell.execute_reply": "2026-01-04T23:38:57.833404Z",
     "shell.execute_reply.started": "2026-01-04T23:38:43.012501Z"
    },
    "id": "5qI9d5wIEJiY",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='pydantic')\n",
    "\n",
    "import gc\n",
    "import math\n",
    "import shutil\n",
    "import glob\n",
    "import random\n",
    "from types import SimpleNamespace\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import cv2\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import torch\n",
    "from torch.optim.swa_utils import update_bn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from lightning.pytorch.callbacks import TQDMProgressBar, Callback\n",
    "\n",
    "from torchmetrics import ConfusionMatrix, AUROC\n",
    "import timm\n",
    "import ttach as tta\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cv2.setNumThreads(0)\n",
    "print(os.cpu_count())\n",
    "torch.set_float32_matmul_precision('high') # L4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    model_arch = 'resnest101e'\n",
    "    is_bn = True\n",
    "    seed = 938\n",
    "    lr = 0.0001\n",
    "    weight_decay = 0.05\n",
    "    alpha = 0.5\n",
    "    # weak_alpha = 0.3\n",
    "    # strong_alpha = 0.7\n",
    "    T = 1.25\n",
    "    drop_path_rate = 0.2\n",
    "    top_k = 5\n",
    "    n_folds = 5\n",
    "    epochs = 27\n",
    "    batch_size = 32\n",
    "    accum_iter = 1\n",
    "    num_workers = 4\n",
    "    persistent_workers=True\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    project_name = 'PlantPathology2020'\n",
    "    exp_name = 'Dist_Model_ver1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "def set_seed(seed, deterministic=False):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed) # cpu\n",
    "    torch.cuda.manual_seed(seed) # gpu\n",
    "    torch.cuda.manual_seed_all(seed) # 멀티 gpu\n",
    "    if deterministic:\n",
    "        os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.use_deterministic_algorithms(True)\n",
    "    else:\n",
    "        torch.backends.cudnn.deterministic = False\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        torch.use_deterministic_algorithms(False)\n",
    "\n",
    "set_seed(CFG.seed)\n",
    "device = CFG.device\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f020cb7c2b0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00_00-plant-Dist_Model.ipynb\n"
     ]
    }
   ],
   "source": [
    "import ipynbname\n",
    "current_file = ipynbname.name() + \".ipynb\"\n",
    "print(current_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dWWDJu8cEJiZ"
   },
   "source": [
    "# 2. Data Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data & Knowledge Source\n",
    "- Teacher Logits:\n",
    "\n",
    "    - Teacher 1: ConvNeXt-Small\n",
    "\n",
    "    - Teacher 2: ConvNeXt-Small variant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logit1 Range: -3.67 ~ 4.77\n",
      "Logit2 Range: -3.80 ~ 5.34\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>fold</th>\n",
       "      <th>group_id</th>\n",
       "      <th>label_idx</th>\n",
       "      <th>healthy</th>\n",
       "      <th>multiple_diseases</th>\n",
       "      <th>rust</th>\n",
       "      <th>scab</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Train_0</td>\n",
       "      <td>3</td>\n",
       "      <td>Train_0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Train_1</td>\n",
       "      <td>4</td>\n",
       "      <td>Train_1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Train_2</td>\n",
       "      <td>0</td>\n",
       "      <td>Train_2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Train_3</td>\n",
       "      <td>3</td>\n",
       "      <td>Train_3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Train_4</td>\n",
       "      <td>2</td>\n",
       "      <td>Train_4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  image_id  fold group_id  label_idx  healthy  multiple_diseases  rust  scab\n",
       "0  Train_0     3  Train_0          3      0.0                0.0   0.0   1.0\n",
       "1  Train_1     4  Train_1          1      0.0                1.0   0.0   0.0\n",
       "2  Train_2     0  Train_2          0      1.0                0.0   0.0   0.0\n",
       "3  Train_3     3  Train_3          2      0.0                0.0   1.0   0.0\n",
       "4  Train_4     2  Train_4          0      1.0                0.0   0.0   0.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DataModule:\n",
    "    def __init__(self, data_dir, logit_path1, logit_path2):\n",
    "        self.data_dir = data_dir\n",
    "        self.img_dir = self.data_dir + 'images/'\n",
    "        self.logit_path1 = logit_path1\n",
    "        self.logit_path2 = logit_path2\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        train_df = pd.read_csv(self.data_dir + 'datasets/train_reborn_02.csv')\n",
    "        train_df = train_df.reset_index(drop=True)\n",
    "        test_df = pd.read_csv(self.data_dir + 'test.csv')\n",
    "        submission = pd.read_csv(self.data_dir + 'sample_submission.csv')\n",
    "        teacher_logit1 = np.load(self.logit_path1)\n",
    "        teacher_logit2 = np.load(self.logit_path2)\n",
    "        teacher_logit = teacher_logit1* 0.5 + teacher_logit2 * 0.5\n",
    "        print(f\"Logit1 Range: {teacher_logit1.min():.2f} ~ {teacher_logit1.max():.2f}\")\n",
    "        print(f\"Logit2 Range: {teacher_logit2.min():.2f} ~ {teacher_logit2.max():.2f}\")\n",
    "        return train_df, teacher_logit, test_df, submission\n",
    "\n",
    "    \n",
    "train_df, teacher_logit, test_df, submission = DataModule(data_dir='../data/',\n",
    " logit_path1='../data/models/s10_convnext_small_T_scheduler/oof_ogit_s10_convnext_small_T_scheduler.npy',\n",
    " logit_path2='../data/models/s12_conv_teacher_s10_v2/oof_logit_s12_conv_teacher_s10_v2.npy').prepare_data()\n",
    "\n",
    "hard_cols = ['healthy', 'multiple_diseases', 'rust', 'scab']\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Load Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading all images into RAM once...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50933323de354c329548d8118925c2d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading Images...:   0%|          | 0/3642 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Images on Ram\n"
     ]
    }
   ],
   "source": [
    "all_images = {}\n",
    "all_img_ids = np.concatenate([train_df['image_id'].tolist(), test_df['image_id'].tolist()])\n",
    "\n",
    "print(\"Loading all images into RAM once...\")\n",
    "for img_id in tqdm(all_img_ids, desc='Loading Images...' ,leave=False):\n",
    "    img = cv2.imread('../data/images/' + img_id + '.jpg')\n",
    "    img = cv2.resize(img, (650, 450))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img.setflags(write=False)\n",
    "    all_images[img_id] = img\n",
    "\n",
    "print(\"All Images on Ram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, df, hard_cols=hard_cols, teacher_logit=None, transform=None, is_test=False):\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        self.is_test = is_test\n",
    "        self.hard_cols = hard_cols\n",
    "        self.teacher_logit = teacher_logit\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.df.iloc[idx, 0]\n",
    "        image = all_images[img_id].copy()\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image=image)['image']\n",
    "\n",
    "        if self.is_test:\n",
    "            return image\n",
    "        else:\n",
    "            hard_labels = self.df.iloc[idx][self.hard_cols].values.astype(np.float32)\n",
    "            oof_logit = self.teacher_logit[idx]\n",
    "            return image, torch.tensor(oof_logit), torch.tensor(hard_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlantDataModule(pl.LightningDataModule):\n",
    "    \"\"\"\n",
    "    데이터 로딩, 전처리 및 학습/검증 세트 분할을 관리하는 클래스입니다.\n",
    "    \n",
    "    K-Fold 인덱스에 따라 데이터를 학습용과 검증용으로 분리하며, \n",
    "    stage 인자에 따라 불필요한 데이터 로딩을 방지하여 메모리 효율성을 최적화합니다. \n",
    "    이미지 증강(Augmentation) 로직을 내부적으로 포함하여 데이터와 모델 사이의 인터페이스를 명확히 정의합니다.\n",
    "    \"\"\"\n",
    "    def __init__(self, train_df, teacher_logit, test_df, cfg, fold_idx=None, inference_mode=False):\n",
    "        super().__init__()\n",
    "        self.train_df = train_df\n",
    "        self.teacher_logit = teacher_logit\n",
    "        self.test_df = test_df\n",
    "        self.cfg = cfg\n",
    "        self.fold_idx = fold_idx\n",
    "        self.inference_mode = inference_mode\n",
    "\n",
    "        self.transform_train = A.Compose([\n",
    "            A.OneOf([\n",
    "                A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0, p=1.0),\n",
    "                A.RandomBrightnessContrast(brightness_limit=0, contrast_limit=0.2, p=1.0)\n",
    "            ], p=1.0),\n",
    "\n",
    "            A.OneOf([\n",
    "                A.MotionBlur(blur_limit=5, p=1.0),\n",
    "                A.MedianBlur(blur_limit=5, p=1.0),\n",
    "                A.GaussianBlur(blur_limit=(3, 5), p=1.0),\n",
    "            ], p=0.5),\n",
    "            \n",
    "            A.OneOf([\n",
    "                A.Affine(\n",
    "                    scale=(0.8, 1.2),       \n",
    "                    translate_percent=0.2,  \n",
    "                    rotate=45,              # 회전 각도 확대\n",
    "                    shear=20,               # 전단 변환(기울기) 추가\n",
    "                    interpolation=cv2.INTER_CUBIC,\n",
    "                    border_mode=cv2.BORDER_REFLECT_101, \n",
    "                    p=1.0\n",
    "                ),\n",
    "                A.Perspective(scale=(0.05, 0.1), p=1.0), # 원근 변환\n",
    "            ], p=0.8),\n",
    "            \n",
    "            # 질감/노이즈\n",
    "            A.OneOf([\n",
    "                A.ISONoise(p=1.0),\n",
    "                A.GaussNoise(p=1.0),\n",
    "            ], p=0.3),\n",
    "\n",
    "            A.CoarseDropout(\n",
    "                num_holes_range=(8, 16),\n",
    "                hole_height_range=(8, 16),\n",
    "                hole_width_range=(8, 16),\n",
    "                fill=[103, 131, 82],\n",
    "                p=0.5\n",
    "            ),\n",
    "\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "\n",
    "            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "\n",
    "        self.transform_test = A.Compose([\n",
    "            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        if stage == 'fit' or stage is None:\n",
    "            if self.fold_idx is None:\n",
    "                print(\"[Info] Full Training Mode Activated!\")\n",
    "                self.train = self.train_df.reset_index(drop=True)\n",
    "                train_logit = self.teacher_logit\n",
    "                self.valid = self.train_df.iloc[:32].reset_index(drop=True)\n",
    "                val_logit = self.teacher_logit[:32]\n",
    "            else:\n",
    "                train_mask = (self.train_df['fold']!=self.fold_idx).values\n",
    "                val_mask = (self.train_df['fold']==self.fold_idx).values\n",
    "\n",
    "                self.train = self.train_df[train_mask].reset_index(drop=True)\n",
    "                self.valid = self.train_df[val_mask].reset_index(drop=True)\n",
    "                train_logit = self.teacher_logit[train_mask]\n",
    "                val_logit = self.teacher_logit[val_mask]\n",
    "\n",
    "            self.dataset_train = ImageDataset(self.train, teacher_logit=train_logit, transform=self.transform_train)\n",
    "            self.dataset_valid = ImageDataset(self.valid, teacher_logit=val_logit, transform=self.transform_test)\n",
    "            print(f'[Fit] Train: {len(self.train)}, Valid: {len(self.valid)}')\n",
    "\n",
    "        elif stage == 'test':\n",
    "            if self.fold_idx is None:\n",
    "                print(\"[Info] Full Training Mode\")\n",
    "                self.dataset_test = ImageDataset(self.test_df, transform=self.transform_test, is_test=True)\n",
    "                print(f'Test: {len(self.test_df)}')\n",
    "            else:\n",
    "                val_mask = (self.train_df['fold'] == self.fold_idx).values\n",
    "                self.valid = self.train_df[val_mask].reset_index(drop=True)\n",
    "                val_logit = self.teacher_logit[val_mask]\n",
    "            \n",
    "                self.dataset_valid = ImageDataset(self.valid, teacher_logit=val_logit, transform=self.transform_test)\n",
    "                self.dataset_test = ImageDataset(self.test_df, transform=self.transform_test, is_test=True)\n",
    "                print(f'[Test] Valid(OOF): {len(self.valid)}, Test: {len(self.test_df)}')\n",
    "\n",
    "        elif stage == 'predict':\n",
    "            self.dataset_test = ImageDataset(self.test_df, transform=self.transform_test, is_test=True)\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        loader_train = DataLoader(self.dataset_train, batch_size=self.cfg.batch_size, shuffle=True,\n",
    "                                worker_init_fn=seed_worker, generator=g, num_workers=self.cfg.num_workers, \n",
    "                                persistent_workers=True, pin_memory=True)\n",
    "        return loader_train\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        user_persistent = not self.inference_mode\n",
    "        loader_valid = DataLoader(self.dataset_valid, batch_size=self.cfg.batch_size*4, shuffle=False,\n",
    "                                worker_init_fn=seed_worker, generator=g, num_workers=self.cfg.num_workers, \n",
    "                                persistent_workers=user_persistent, pin_memory=True)\n",
    "        return loader_valid\n",
    "    \n",
    "    def predict_dataloader(self):\n",
    "        loader_test = DataLoader(self.dataset_test, batch_size=self.cfg.batch_size*4, shuffle=False,\n",
    "                                worker_init_fn=seed_worker, generator=g, num_workers=self.cfg.num_workers, \n",
    "                                persistent_workers=False, pin_memory=True)\n",
    "        return loader_test\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return self.predict_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oohzJHhyEJic"
   },
   "source": [
    "# 3. Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FoldAlphaCallback(Callback):\n",
    "    \"\"\"\n",
    "    폴드별로 Knowledge Distillation의 Alpha 값을 다르게 조정하는 콜백\n",
    "    - 성능이 낮은 폴드: GT 비중(Alpha)을 높여 Teacher 의존도 낮춤\n",
    "    - 성능이 높은 폴드: Teacher 비중(1-Alpha)을 높여 정규화 효과 극대화\n",
    "    \"\"\"\n",
    "    def __init__(self, current_fold, weak_folds=[0, 1], weak_alpha=0.8, strong_alpha=0.5):\n",
    "        super().__init__()\n",
    "        self.current_fold = current_fold\n",
    "        self.weak_folds = weak_folds\n",
    "        self.weak_alpha = weak_alpha\n",
    "        self.strong_alpha = strong_alpha\n",
    "\n",
    "    def on_train_start(self, trainer, pl_module):\n",
    "        # 학습 시작 시점에 폴드에 맞는 Alpha 설정\n",
    "        if self.current_fold in self.weak_folds:\n",
    "            pl_module.current_alpha = self.weak_alpha\n",
    "            strategy = \"GT Focus (Weak Fold)\"\n",
    "        else:\n",
    "            pl_module.current_alpha = self.strong_alpha\n",
    "            strategy = \"Regularization (Strong Fold)\"\n",
    "            \n",
    "        # 로그 출력 (한 번만)\n",
    "        if trainer.global_rank == 0:\n",
    "            print(f\"\\n[FoldAlphaCallback] Fold {self.current_fold+1}: \"\n",
    "                  f\"Alpha set to {pl_module.current_alpha} ({strategy})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlantDiseaseModule(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    모델의 순전파, 손실 함수 계산, 최적화 알고리즘 및 메트릭 측정을 캡슐화합니다.\n",
    "    특히 훈련 단계에서는 Soft Label Mixing(Knowledge Distillation 원리 적용)을 통해 \n",
    "    라벨 노이즈에 대한 강건성을 확보하며, 추론 단계에서는 TTA(Test Time Augmentation)를 \n",
    "    통합하여 예측의 불확실성을 줄이고 일반화 성능을 향상시킵니다.\n",
    "    \"\"\"\n",
    "    def __init__(self, config, steps_per_epoch=None):\n",
    "        super().__init__()\n",
    "        if isinstance(config, type):\n",
    "            config = {k: v for k, v in config.__dict__.items() if not k.startswith('__')}\n",
    "        self.save_hyperparameters(config)\n",
    "        self.current_alpha = self.hparams.alpha\n",
    "        self.model = timm.create_model(\n",
    "            self.hparams.model_arch,\n",
    "            pretrained=True,\n",
    "            drop_path_rate=self.hparams.drop_path_rate,\n",
    "            num_classes=4\n",
    "            )\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "        # TTA\n",
    "        self.tta_transforms = tta.Compose([\n",
    "                tta.HorizontalFlip(),\n",
    "                tta.VerticalFlip(),\n",
    "            ])\n",
    "        \n",
    "        # metrics\n",
    "        self.valid_auc = AUROC(task='multiclass', num_classes=4)\n",
    "        self.valid_cm = ConfusionMatrix(task='multiclass', num_classes=4)\n",
    "        self.best_score = 0.0\n",
    "\n",
    "        self.top_k_scores = []  # (score, epoch) 튜플을 저장할 리스트\n",
    "        self.top_k = self.hparams.top_k\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.hparams.lr, weight_decay=self.hparams.weight_decay)\n",
    "        \n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=9, T_mult=2, eta_min=1e-6)\n",
    "        \n",
    "        scheduler_config = {\n",
    "            'scheduler' : scheduler,\n",
    "            'interval' : 'epoch',\n",
    "            'frequency' : 1\n",
    "        }\n",
    "        \n",
    "        return [optimizer], [scheduler_config]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        T = self.hparams.T\n",
    "        alpha = self.current_alpha\n",
    "\n",
    "        image, logit_from_oof, hard_labels = batch\n",
    "        outputs = self.model(image)\n",
    "\n",
    "        # 하드라벨 손실\n",
    "        loss_hard = self.criterion(outputs, hard_labels)\n",
    "\n",
    "        # Teacher\n",
    "        teacher_probs = torch.softmax(logit_from_oof / T, dim=1)\n",
    "        # Student\n",
    "        student_log_probs = torch.nn.functional.log_softmax(outputs / T, dim=1)\n",
    "\n",
    "        kl_loss = torch.nn.functional.kl_div(\n",
    "            student_log_probs, \n",
    "            teacher_probs, \n",
    "            reduction='batchmean'\n",
    "        )\n",
    "        loss_soft = kl_loss * (T**2)\n",
    "        # total loss\n",
    "        loss = alpha * loss_hard + (1 - alpha) * loss_soft\n",
    "        # log\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('hard_loss', loss_hard, on_step=False, on_epoch=True, logger=True)\n",
    "        self.log('soft_loss', loss_soft, on_step=False, on_epoch=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        image, _, hard_labels = batch\n",
    "        outputs = self.model(image)\n",
    "        loss = self.criterion(outputs, hard_labels)\n",
    "        \n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        targets = torch.argmax(hard_labels, dim=1)\n",
    "\n",
    "        self.valid_cm(preds, targets)\n",
    "        self.valid_auc(probs, targets)\n",
    "        \n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('val_roc_auc', self.valid_auc, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        if self.trainer.sanity_checking:\n",
    "            return\n",
    "                  \n",
    "        score = self.trainer.callback_metrics.get('val_roc_auc')\n",
    "        current_epoch = self.current_epoch\n",
    "        \n",
    "        if score is not None:\n",
    "            current_score = score.item()            \n",
    "            self.top_k_scores.append((current_score, current_epoch))\n",
    "            self.top_k_scores.sort(key=lambda x: x[0], reverse=True)\n",
    "            self.top_k_scores = self.top_k_scores[:self.top_k]\n",
    "            is_in_top_k = (current_score, current_epoch) in self.top_k_scores\n",
    "            \n",
    "            if is_in_top_k and isinstance(self.logger, WandbLogger):\n",
    "                rank = self.top_k_scores.index((current_score, current_epoch)) + 1\n",
    "                top_k_str = \", \".join([f\"(Ep {e}: {s:.4f})\" for s, e in self.top_k_scores])\n",
    "                self.print(f\"Current Top-{self.top_k}: {top_k_str}\")\n",
    "                \n",
    "                cm = self.valid_cm.compute().cpu().numpy()\n",
    "                columns = ['Healthy', 'Multiple', 'Rust', 'Scab']\n",
    "                plt.figure(figsize=(10, 8))\n",
    "                sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                            xticklabels=columns, yticklabels=columns,\n",
    "                            annot_kws={\"size\": 14})\n",
    "                            \n",
    "                plt.ylabel('True Label', fontsize=12)\n",
    "                plt.xlabel('Predicted Label', fontsize=12)\n",
    "                plt.title(f'Confusion Matrix (Epoch {current_epoch})', fontsize=14)\n",
    "             \n",
    "                self.logger.experiment.log({\n",
    "                    \"val/confusion_matrix\": wandb.Image(plt, caption=f\"Epoch {current_epoch}\"),\n",
    "                    \"global_step\": self.global_step\n",
    "                })\n",
    "                plt.close()\n",
    "\n",
    "        self.valid_cm.reset()\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        score = self.trainer.callback_metrics.get('val_roc_auc')\n",
    "        train_loss = self.trainer.callback_metrics.get('train_loss_epoch')\n",
    "        val_loss = self.trainer.callback_metrics.get('val_loss')\n",
    "\n",
    "        current_epoch = self.current_epoch\n",
    "        t_loss_str = f\"{train_loss:.4f}\" if train_loss is not None else \"N/A\"\n",
    "        v_loss_str = f\"{val_loss:.4f}\" if val_loss is not None else \"N/A\"\n",
    "        roc_str = f\"{score:.4f}\" if score is not None else \"N/A\"\n",
    "        self.print(f\"\\n(Epoch {current_epoch}) Train Loss: {t_loss_str} | Val Loss: {v_loss_str} | ROC AUC: {roc_str}\")        \n",
    "\n",
    "    def on_predict_start(self):\n",
    "        self.tta_model = tta.ClassificationTTAWrapper(\n",
    "            self.model, \n",
    "            self.tta_transforms, \n",
    "            merge_mode='mean'\n",
    "        )\n",
    "\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        if isinstance(batch, (list, tuple)):\n",
    "            x = batch[0]\n",
    "        else:\n",
    "            x = batch\n",
    "        \n",
    "        outputs = self.tta_model(x)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "186"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Training Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Metrics & Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "OdZxEQE7yhZy"
   },
   "outputs": [],
   "source": [
    "class MetricHandler:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.preds_list = []\n",
    "        self.actual_list = []\n",
    "\n",
    "    def update(self, preds, actual):\n",
    "        self.preds_list.extend(preds)\n",
    "        self.actual_list.extend(actual)\n",
    "\n",
    "    def compute_roc_auc(self):\n",
    "        return roc_auc_score(self.actual_list, self.preds_list)\n",
    "    \n",
    "    \n",
    "class BackupHandler:\n",
    "    def __init__(self, local_dir, backup_dir=None, active=True):\n",
    "        self.local_dir = local_dir\n",
    "        self.backup_dir = backup_dir\n",
    "        self.active = active and (backup_dir is not None)\n",
    "\n",
    "        if self.active and self.backup_dir is not None:\n",
    "            os.makedirs(self.backup_dir, exist_ok=True)\n",
    "            print(f'Backup Active : {self.local_dir} -> {self.backup_dir}')\n",
    "\n",
    "    def backup(self, filename):\n",
    "        if not self.active or self.backup_dir is None:\n",
    "            return\n",
    "\n",
    "        src_path = os.path.join(self.local_dir, filename)\n",
    "        dst_path = os.path.join(self.backup_dir, filename)\n",
    "        \n",
    "        if os.path.exists(src_path):\n",
    "            shutil.copy(src_path, dst_path)\n",
    "\n",
    "    def save_file(self, data, filename, logit=False):\n",
    "        local_path = os.path.join(self.local_dir, filename)\n",
    "\n",
    "        if logit:\n",
    "            np.save(local_path, data)\n",
    "            print(f'Logit saved at {local_path}')\n",
    "\n",
    "        else:\n",
    "            data.to_csv(local_path, index=False)\n",
    "            print(f'CSV saved at {local_path}')\n",
    "\n",
    "        self.backup(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Experiment Ochestrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentRunner:\n",
    "    \"\"\"\n",
    "    K-Fold 교차 검증 및 전체 실험 프로세스를 지휘하는 오케스트레이터 클래스입니다.\n",
    "    \n",
    "    환경 설정(Kaggle, Colab, Local)에 따른 경로 자동화부터 WandB 로깅, 체크포인트 저장, \n",
    "    K-Fold 학습 루프 제어 및 최종 추론(OOF 및 Test)까지의 전체 워크플로우를 담당합니다.\n",
    "    실험이 종료될 때마다 명시적인 메모리 정리(GC, CUDA Cache)를 수행하여 \n",
    "    리소스 사용을 최적화하고 연속적인 실험 안정성을 보장합니다.\n",
    "    \"\"\"\n",
    "    def __init__(self, config, train_df, teacher_logit, test_df):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.train_df = train_df\n",
    "        self.teacher_logit = teacher_logit\n",
    "        self.test_df = test_df\n",
    "        self.paths = self._setup_env()\n",
    "        self.backup_handler = BackupHandler(local_dir=self.paths.local_path , backup_dir=self.paths.drive_path, active=False)\n",
    "        \n",
    "    def _setup_env(self):\n",
    "        is_kaggle = os.path.exists('/kaggle/') \n",
    "        is_colab = os.path.exists('/content/drive/Mydrive') and not is_kaggle\n",
    "\n",
    "        if is_kaggle:\n",
    "            print(\"Environment: Kaggle\")\n",
    "            drive_path = None\n",
    "            local_path = '/kaggle/working/'\n",
    "        elif is_colab:\n",
    "            print(\"Environment: Google Colab\")\n",
    "            drive_path = f'/content/drive/MyDrive/Kaggle_Save/{CFG.exp_name}/'\n",
    "            local_path = '/content/models/'\n",
    "        else:\n",
    "            print(\"Environment: Local\")\n",
    "            drive_path = None\n",
    "            local_path = f'../data/models/{CFG.exp_name}/'\n",
    "        \n",
    "        print(f\"Save Path: {local_path}\")\n",
    "        return SimpleNamespace(local_path=local_path, drive_path=drive_path)    \n",
    "\n",
    "    def _get_callbacks(self, fold_idx):\n",
    "        callbacks = []\n",
    "        progress_bar = TQDMProgressBar(refresh_rate=1)\n",
    "        callbacks.append(progress_bar)\n",
    "\n",
    "        if fold_idx is None:  # Full Training\n",
    "            ckpt_callback = pl.callbacks.ModelCheckpoint(\n",
    "                monitor='epoch',\n",
    "                mode='max',\n",
    "                save_top_k=self.config.top_k, # 마지막 k개 에폭 저장\n",
    "                save_last=False,\n",
    "                dirpath=self.paths.local_path,\n",
    "                filename='Full-Ep{epoch:02d}',\n",
    "                auto_insert_metric_name=False,\n",
    "            )\n",
    "\n",
    "        else:  # K-Fold Training\n",
    "            ckpt_callback = pl.callbacks.ModelCheckpoint(\n",
    "                monitor='val_roc_auc',\n",
    "                mode='max',\n",
    "                save_top_k=self.config.top_k,\n",
    "                save_weights_only=True,\n",
    "                save_last=False,\n",
    "                dirpath=self.paths.local_path,\n",
    "                filename=f'Fold{fold_idx+1}-Ep{{epoch:02d}}-{{val_roc_auc:.4f}}',\n",
    "                auto_insert_metric_name=False,\n",
    "            )\n",
    "            \n",
    "            # Fold별 Alpha 전략 적용\n",
    "            alpha_callback = FoldAlphaCallback(\n",
    "                current_fold=fold_idx,\n",
    "                weak_folds=[0, 1],\n",
    "                weak_alpha=self.config.weak_alpha ,\n",
    "                strong_alpha=self.config.strong_alpha,\n",
    "            )\n",
    "            callbacks.append(alpha_callback)\n",
    "\n",
    "        callbacks.insert(0, ckpt_callback) # 체크포인트를 맨 앞에\n",
    "        return callbacks, ckpt_callback\n",
    "\n",
    "    def _run_single_training(self, fold_idx=None):\n",
    "        mode_name = \"Full Train\" if fold_idx is None else f\"Fold_{fold_idx+1}\"\n",
    "        print('='*30, f'{mode_name} START', '='*30)\n",
    "\n",
    "        # Logger 설정\n",
    "        wandb_logger = WandbLogger(\n",
    "            project=self.config.project_name,\n",
    "            group=self.config.exp_name,\n",
    "            settings=wandb.Settings(program=current_file),\n",
    "            name=mode_name,\n",
    "            job_type=\"train\",\n",
    "            save_code=True,\n",
    "            config={k: v for k, v in self.config.__dict__.items() if not k.startswith('__')}\n",
    "        )\n",
    "\n",
    "        # DataModule\n",
    "        datamodule = PlantDataModule(\n",
    "            train_df=self.train_df, \n",
    "            teacher_logit=self.teacher_logit, \n",
    "            test_df=self.test_df, \n",
    "            cfg=self.config, \n",
    "            fold_idx=fold_idx\n",
    "        )\n",
    "\n",
    "        # 모델 초기화\n",
    "        model = PlantDiseaseModule(self.config)\n",
    "        if fold_idx is None:\n",
    "             model.current_alpha = self.config.alpha\n",
    "\n",
    "        # Callbacks 가져오기\n",
    "        callbacks, ckpt_callback = self._get_callbacks(fold_idx)\n",
    "\n",
    "        # Trainer 설정\n",
    "        trainer = pl.Trainer(\n",
    "            max_epochs=self.config.epochs,\n",
    "            accelerator='auto',\n",
    "            precision='16-mixed',\n",
    "            accumulate_grad_batches=self.config.accum_iter,\n",
    "            callbacks=callbacks,\n",
    "            logger=wandb_logger,\n",
    "            log_every_n_steps=10,\n",
    "            num_sanity_val_steps=0\n",
    "        )\n",
    "\n",
    "        trainer.fit(model, datamodule=datamodule)\n",
    "        \n",
    "        if fold_idx is not None:\n",
    "            print(f'\\n Top-{ckpt_callback.save_top_k} Models in this Fold:')\n",
    "            for path, score in ckpt_callback.best_k_models.items():\n",
    "                print(f'> {os.path.basename(path)}')\n",
    "\n",
    "        wandb.finish()\n",
    "        \n",
    "        # 메모리 정리\n",
    "        del datamodule, trainer, model\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"K-Fold 학습 실행\"\"\"\n",
    "        for fold in range(self.config.n_folds):\n",
    "            self._run_single_training(fold_idx=fold)\n",
    "\n",
    "    def run_full_training(self):\n",
    "        \"\"\"전체 데이터 학습 실행\"\"\"\n",
    "        self._run_single_training(fold_idx=None)\n",
    "        print(\"Full training completed.\")\n",
    "\n",
    "    def average_checkpoints(self, checkpoint_paths, output_path):\n",
    "        print(f\"Averaging {len(checkpoint_paths)} checkpoints...\")\n",
    "        avg_state_dict = {}\n",
    "        \n",
    "        # 첫 번째 모델 로드\n",
    "        first_ckpt = torch.load(checkpoint_paths[0], map_location='cpu')['state_dict']\n",
    "        for k, v in first_ckpt.items():\n",
    "            if v.is_floating_point():\n",
    "                avg_state_dict[k] = v.float()\n",
    "            else:\n",
    "                avg_state_dict[k] = v\n",
    "                \n",
    "        # 나머지 모델들과 합산\n",
    "        for i in range(1, len(checkpoint_paths)):\n",
    "            ckpt = torch.load(checkpoint_paths[i], map_location='cpu')['state_dict']\n",
    "            for k in avg_state_dict:\n",
    "                if avg_state_dict[k].is_floating_point():\n",
    "                    avg_state_dict[k] += ckpt[k].float()\n",
    "        \n",
    "        # 나누기\n",
    "        for k in avg_state_dict:\n",
    "            if avg_state_dict[k].is_floating_point():\n",
    "                avg_state_dict[k] /= len(checkpoint_paths)\n",
    "                \n",
    "        model = PlantDiseaseModule(self.config)\n",
    "        model.load_state_dict(avg_state_dict)\n",
    "        if self.config.is_bn:\n",
    "            model = self.run_bn(model=model, save_path=output_path, fold_idx=None)\n",
    "        torch.save(model.state_dict(), output_path)\n",
    "        print(f\"Averaged model saved to: {output_path}\")\n",
    "        return model\n",
    "\n",
    "    def _load_averaged_model(self, fold):\n",
    "        save_path = os.path.join(self.paths.local_path, f'best_score_model_{fold+1}.pth')\n",
    "        model = PlantDiseaseModule(self.config)\n",
    "\n",
    "        if os.path.exists(save_path):\n",
    "            print(f'Found existing averaged model for Fold {fold+1}. Loading directly...')\n",
    "            state_dict = torch.load(save_path, map_location=device)\n",
    "            model.load_state_dict(state_dict)\n",
    "        else:\n",
    "            print(f'Merging Top-K Models for Fold {fold+1} ...')\n",
    "            score_pattern = os.path.join(self.paths.local_path, f'Fold{fold+1}-Ep*.ckpt')\n",
    "            score_files = glob.glob(score_pattern)\n",
    "            print(f'Found {len(score_files)} score models : {[os.path.basename(f) for f in score_files]}')\n",
    "            \n",
    "            first_state = torch.load(score_files[0], map_location='cpu')['state_dict']\n",
    "            avg_state_dict = {}\n",
    "            for k, v in first_state.items():\n",
    "                if v.is_floating_point():\n",
    "                    avg_state_dict[k] = v.float() # Float32로 변환하여 초기화\n",
    "                else:\n",
    "                    avg_state_dict[k] = v \n",
    "            \n",
    "            if len(score_files) > 1:\n",
    "                for path in score_files[1:]:\n",
    "                    state_dict = torch.load(path, map_location='cpu')['state_dict']\n",
    "                    for key in avg_state_dict:\n",
    "                        if avg_state_dict[key].is_floating_point():\n",
    "                            avg_state_dict[key] += state_dict[key].float()\n",
    "                        else:\n",
    "                            pass\n",
    "                for key in avg_state_dict:\n",
    "                    if avg_state_dict[key].is_floating_point():\n",
    "                        avg_state_dict[key] = avg_state_dict[key] / len(score_files)\n",
    "            \n",
    "            model.load_state_dict(avg_state_dict)\n",
    "            \n",
    "            for remove_path in score_files:\n",
    "                if os.path.exists(remove_path):\n",
    "                    os.remove(remove_path)\n",
    "            \n",
    "            save_path = os.path.join(self.paths.local_path, f'best_score_model_{fold+1}.pth')\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print('Save Avg Model : ', save_path)\n",
    "\n",
    "        if self.config.is_bn:\n",
    "            model = self.run_bn(model=model, save_path=save_path, fold_idx=fold)\n",
    "        else:\n",
    "            print('Skipping BN update for LayerNorm')\n",
    "            model = model.to(self.config.device)\n",
    "        return model\n",
    "\n",
    "    def run_bn(self, model, save_path, fold_idx=None): \n",
    "        \"\"\"\n",
    "        Model Soup(가중치 평균) 이후, 망가진 Batch Normalization 통계치를\n",
    "        현재 데이터 기준으로 재계산하고 모델 성능을 복구 및 저장합니다.\n",
    "        \"\"\"\n",
    "        mode_label = \"Full Data\" if fold_idx is None else f\"Fold {fold_idx+1}\"\n",
    "        print(f'Update BN stats ({mode_label}) ... ')\n",
    "        \n",
    "        model = model.to(self.config.device)\n",
    "        model.train() \n",
    "\n",
    "        # [데이터 선택]\n",
    "        if fold_idx is None:\n",
    "            train_subset = self.train_df.reset_index(drop=True)\n",
    "        else: # K-Fold 모드\n",
    "            train_subset = self.train_df[self.train_df['fold'] != fold_idx].reset_index(drop=True)\n",
    "        \n",
    "        transform_test = A.Compose([\n",
    "            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "        \n",
    "        dataset_bn = ImageDataset(train_subset, transform=transform_test, is_test=True)\n",
    "        loader_bn = DataLoader(\n",
    "            dataset_bn, \n",
    "            batch_size=self.config.batch_size, \n",
    "            shuffle=True,\n",
    "            num_workers=self.config.num_workers,\n",
    "            pin_memory=True,\n",
    "            drop_last=True\n",
    "        )\n",
    "        \n",
    "        update_bn(loader_bn, model, device=self.config.device)\n",
    "        \n",
    "        model.eval()\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        print(f\"BN updated model saved to: {save_path}\")\n",
    "        return model\n",
    "\n",
    "    def find_optimal_temperature(self, logits, labels):\n",
    "        \"\"\"\n",
    "        OOF Logits와 정답을 이용해 NLL을 최소화하는 T 값 탐색\n",
    "        \"\"\"\n",
    "        # 정답 라벨 처리 (One-hot -> Index)\n",
    "        if labels.ndim > 1:\n",
    "            labels = np.argmax(labels, axis=1)\n",
    "        \n",
    "        logits_tensor = torch.tensor(logits, dtype=torch.float32)\n",
    "        labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "        \n",
    "        # NLL Loss\n",
    "        t_candidates = np.arange(0.5, 2.6, 0.1)\n",
    "        best_t = min(t_candidates, key=lambda t: torch.nn.CrossEntropyLoss()(logits_tensor / t, labels_tensor).item())\n",
    "        print(f\"    > Best T: {best_t:.1f}\")\n",
    "        return best_t\n",
    "\n",
    "    def _predict_step(self, model, datamodule, dataloader_type='val'):\n",
    "        progress_bar = TQDMProgressBar(refresh_rate=1)\n",
    "        infer_trainer = pl.Trainer(\n",
    "            accelerator='auto',\n",
    "            precision='16-mixed',\n",
    "            logger=False,\n",
    "            enable_checkpointing=False,\n",
    "            callbacks=[progress_bar]\n",
    "        )\n",
    "        \n",
    "        if dataloader_type == 'val':\n",
    "            dataloader = datamodule.val_dataloader()\n",
    "        else:\n",
    "            dataloader = datamodule.predict_dataloader()\n",
    "            \n",
    "        preds_list = infer_trainer.predict(model, dataloaders=dataloader)\n",
    "        preds_logit = torch.cat(preds_list).cpu().numpy()\n",
    "        \n",
    "        del infer_trainer\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        return preds_logit\n",
    "\n",
    "    def run_inference(self, full=False):\n",
    "        oof_preds = np.zeros((len(self.train_df), 4))\n",
    "        oof_preds_og = np.zeros((len(self.train_df), 4))\n",
    "        oof_preds_logit = np.zeros((len(self.train_df), 4))\n",
    "\n",
    "        final_preds = np.zeros((len(self.test_df), 4))\n",
    "        final_preds_og = np.zeros((len(self.test_df), 4))\n",
    "\n",
    "        if full:\n",
    "            print(f'=== Full Inference (Model Soup) ===')\n",
    "            best_model_paths = sorted(glob.glob(os.path.join(self.paths.local_path, \"Full-Ep*.ckpt\")))\n",
    "            target_checkpoints = best_model_paths[-5:]\n",
    "            final_save_path = os.path.join(self.paths.local_path, \"final_soup_model.pth\")    \n",
    "            avg_model = self.average_checkpoints(target_checkpoints, final_save_path)\n",
    "\n",
    "            infer_module = PlantDataModule(train_df=self.train_df, teacher_logit=self.teacher_logit, test_df=self.test_df, cfg=self.config, fold_idx=None, inference_mode=True)\n",
    "            infer_module.setup(stage='test')\n",
    "            \n",
    "            current_test_logits = self._predict_step(avg_model, infer_module, dataloader_type='test')\n",
    "            test_probs = torch.softmax(torch.tensor(current_test_logits), dim=1).numpy()\n",
    "            final_preds_og = test_probs\n",
    "\n",
    "            return final_preds_og\n",
    "\n",
    "        else: # K-Fold Inference\n",
    "            for fold in range(self.config.n_folds):\n",
    "                print(f'=== Inference Fold {fold+1} ===')\n",
    "                avg_model = self._load_averaged_model(fold)\n",
    "\n",
    "                infer_module = PlantDataModule(train_df=self.train_df, teacher_logit=self.teacher_logit, test_df=self.test_df, cfg=self.config, fold_idx=fold, inference_mode=True)\n",
    "                infer_module.setup(stage='test')\n",
    "\n",
    "                # OOF 예측\n",
    "                current_oof_logits = self._predict_step(avg_model, infer_module, dataloader_type='val')\n",
    "                \n",
    "                # Test 예측\n",
    "                current_test_logits = self._predict_step(avg_model, infer_module, dataloader_type='test')\n",
    "\n",
    "                # 검증 및 Calibration\n",
    "                valid_indices = self.train_df[self.train_df['fold'] == fold].index.values\n",
    "                valid_labels = self.train_df.iloc[valid_indices][hard_cols].values\n",
    "                \n",
    "                optimal_t = self.find_optimal_temperature(current_oof_logits, valid_labels)\n",
    "                \n",
    "                oof_preds[valid_indices] = torch.softmax(torch.tensor(current_oof_logits) / optimal_t, dim=1).numpy()\n",
    "                oof_preds_og[valid_indices] = torch.softmax(torch.tensor(current_oof_logits), dim=1).numpy()\n",
    "                oof_preds_logit[valid_indices] = current_oof_logits\n",
    "                \n",
    "                final_preds += torch.softmax(torch.tensor(current_test_logits) / optimal_t, dim=1).numpy()\n",
    "                final_preds_og += torch.softmax(torch.tensor(current_test_logits), dim=1).numpy()\n",
    "\n",
    "                del avg_model, infer_module\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "\n",
    "            final_preds /= self.config.n_folds\n",
    "            final_preds_og /= self.config.n_folds\n",
    "            \n",
    "            metric_handler = MetricHandler()\n",
    "            metric_handler.update(oof_preds, self.train_df[hard_cols].values)\n",
    "            oof_roc = metric_handler.compute_roc_auc()\n",
    "            print(f'\\n>>> Final OOF ROC AUC : {oof_roc:.5f}')\n",
    "            return oof_preds, oof_preds_og, oof_preds_logit, final_preds, final_preds_og"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Training Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment: Local\n",
      "Save Path: ../data/models/Dist_Model_ver1/\n"
     ]
    }
   ],
   "source": [
    "runner = ExperimentRunner(config=CFG, train_df=train_df, teacher_logit=teacher_logit, test_df=test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================== Full Train START ==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The anonymous setting has no effect and will be removed in a future version.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>wandb/run-20260119_085757-pdbjs4kk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/paraise-/PlantPathology2020/runs/pdbjs4kk' target=\"_blank\">Full Train</a></strong> to <a href='https://wandb.ai/paraise-/PlantPathology2020' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/paraise-/PlantPathology2020' target=\"_blank\">https://wandb.ai/paraise-/PlantPathology2020</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/paraise-/PlantPathology2020/runs/pdbjs4kk' target=\"_blank\">https://wandb.ai/paraise-/PlantPathology2020/runs/pdbjs4kk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Full Training Mode Activated!\n",
      "[Fit] Train: 1821, Valid: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/lightning/pytorch/utilities/model_summary/model_summary.py:242: Precision 16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name      </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type                      </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> FLOPs </span>┃\n",
       "┡━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ model     │ ResNet                    │ 46.2 M │ train │     0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ criterion │ CrossEntropyLoss          │      0 │ train │     0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ valid_auc │ MulticlassAUROC           │      0 │ train │     0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ valid_cm  │ MulticlassConfusionMatrix │      0 │ train │     0 │\n",
       "└───┴───────────┴───────────────────────────┴────────┴───────┴───────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName     \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType                     \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mFLOPs\u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ model     │ ResNet                    │ 46.2 M │ train │     0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ criterion │ CrossEntropyLoss          │      0 │ train │     0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ valid_auc │ MulticlassAUROC           │      0 │ train │     0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ valid_cm  │ MulticlassConfusionMatrix │      0 │ train │     0 │\n",
       "└───┴───────────┴───────────────────────────┴────────┴───────┴───────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 46.2 M                                                                                           \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 46.2 M                                                                                               \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 184                                                                        \n",
       "<span style=\"font-weight: bold\">Modules in train mode</span>: 734                                                                                         \n",
       "<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total FLOPs</span>: 0                                                                                                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 46.2 M                                                                                           \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 46.2 M                                                                                               \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 184                                                                        \n",
       "\u001b[1mModules in train mode\u001b[0m: 734                                                                                         \n",
       "\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal FLOPs\u001b[0m: 0                                                                                                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4015f52b97741edb640b0710a9b0ef2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32c768fd9df148f0b30f1f17176ded3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Top-5: (Ep 0: 0.9815)\n",
      "\n",
      "(Epoch 0) Train Loss: 0.7629 | Val Loss: 0.3840 | ROC AUC: 0.9815\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bcb41e8064141049bed76211218f6c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Top-5: (Ep 1: 0.9951), (Ep 0: 0.9815)\n",
      "\n",
      "(Epoch 1) Train Loss: 0.3876 | Val Loss: 0.2561 | ROC AUC: 0.9951\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63438046d1a147b1b7e45869fce54104",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Top-5: (Ep 1: 0.9951), (Ep 0: 0.9815), (Ep 2: 0.9754)\n",
      "\n",
      "(Epoch 2) Train Loss: 0.3006 | Val Loss: 0.4100 | ROC AUC: 0.9754\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60a14e70dc3d4219b0cfd4d7be727da1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Top-5: (Ep 3: 0.9990), (Ep 1: 0.9951), (Ep 0: 0.9815), (Ep 2: 0.9754)\n",
      "\n",
      "(Epoch 3) Train Loss: 0.2692 | Val Loss: 0.1189 | ROC AUC: 0.9990\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c5e0b1c41bc471e985626ddd5fd751b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Top-5: (Ep 4: 1.0000), (Ep 3: 0.9990), (Ep 1: 0.9951), (Ep 0: 0.9815), (Ep 2: 0.9754)\n",
      "\n",
      "(Epoch 4) Train Loss: 0.2476 | Val Loss: 0.1429 | ROC AUC: 1.0000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f88240b47a394dcca4bdb79d637c1a3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Top-5: (Ep 4: 1.0000), (Ep 3: 0.9990), (Ep 1: 0.9951), (Ep 5: 0.9923), (Ep 0: 0.9815)\n",
      "\n",
      "(Epoch 5) Train Loss: 0.2208 | Val Loss: 0.1520 | ROC AUC: 0.9923\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07beaae62f3c4c34951ad190ccca7f71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Top-5: (Ep 4: 1.0000), (Ep 3: 0.9990), (Ep 6: 0.9961), (Ep 1: 0.9951), (Ep 5: 0.9923)\n",
      "\n",
      "(Epoch 6) Train Loss: 0.2098 | Val Loss: 0.1277 | ROC AUC: 0.9961\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53b293ec904f42fbbcd825722bd81303",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Top-5: (Ep 4: 1.0000), (Ep 7: 1.0000), (Ep 3: 0.9990), (Ep 6: 0.9961), (Ep 1: 0.9951)\n",
      "\n",
      "(Epoch 7) Train Loss: 0.1914 | Val Loss: 0.1334 | ROC AUC: 1.0000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c28a64177d648169bf6cc02f95a1791",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Top-5: (Ep 4: 1.0000), (Ep 7: 1.0000), (Ep 8: 1.0000), (Ep 3: 0.9990), (Ep 6: 0.9961)\n",
      "\n",
      "(Epoch 8) Train Loss: 0.1793 | Val Loss: 0.1352 | ROC AUC: 1.0000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "666d4879b26c45cc9db61c1dc54362bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(Epoch 9) Train Loss: 0.2521 | Val Loss: 0.1612 | ROC AUC: 0.9897\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6f65e22520e44e49c90b29665b03dd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Top-5: (Ep 4: 1.0000), (Ep 7: 1.0000), (Ep 8: 1.0000), (Ep 3: 0.9990), (Ep 10: 0.9990)\n",
      "\n",
      "(Epoch 10) Train Loss: 0.2612 | Val Loss: 0.1560 | ROC AUC: 0.9990\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "923b83d5cad94df6b88e867836c4e8e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Top-5: (Ep 4: 1.0000), (Ep 7: 1.0000), (Ep 8: 1.0000), (Ep 11: 1.0000), (Ep 3: 0.9990)\n",
      "\n",
      "(Epoch 11) Train Loss: 0.2424 | Val Loss: 0.0989 | ROC AUC: 1.0000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8275a3ae1fa4fc584fbb0c8fba2bc5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(Epoch 12) Train Loss: 0.2370 | Val Loss: 0.1668 | ROC AUC: 0.9990\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8e7854043234513b12d28360916c028",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(Epoch 13) Train Loss: 0.1906 | Val Loss: 0.2830 | ROC AUC: 0.9903\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1422e1387e194c59b6469061d4d24109",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(Epoch 14) Train Loss: 0.1965 | Val Loss: 0.2758 | ROC AUC: 0.9889\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "647da95d73aa438fa731d7ccecd0ce12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(Epoch 15) Train Loss: 0.1729 | Val Loss: 0.2312 | ROC AUC: 0.9874\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f2a1d45cb2b47bfb72b46d7a23c237b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Top-5: (Ep 4: 1.0000), (Ep 7: 1.0000), (Ep 8: 1.0000), (Ep 11: 1.0000), (Ep 16: 1.0000)\n",
      "\n",
      "(Epoch 16) Train Loss: 0.1819 | Val Loss: 0.1351 | ROC AUC: 1.0000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a323793f1a844c52b9d90025a9b7128a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(Epoch 17) Train Loss: 0.1581 | Val Loss: 0.2291 | ROC AUC: 0.9932\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa0ebd3ba7ba482ca4fafc9d43b490e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(Epoch 18) Train Loss: 0.1739 | Val Loss: 0.2085 | ROC AUC: 0.9946\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf0e7483e2814b8c97277d2ed1c68246",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(Epoch 19) Train Loss: 0.1534 | Val Loss: 0.2533 | ROC AUC: 0.9970\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98d5881e7642489b853dabebbe74f7b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(Epoch 20) Train Loss: 0.1234 | Val Loss: 0.2358 | ROC AUC: 0.9980\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57dc830c4f7449248b93c8ce691d6877",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(Epoch 21) Train Loss: 0.1283 | Val Loss: 0.2766 | ROC AUC: 0.9980\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5de4221f5bcf4f36bae228714c564d8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(Epoch 22) Train Loss: 0.1236 | Val Loss: 0.2437 | ROC AUC: 0.9960\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4986b666baec4807b232d39cce344f79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(Epoch 23) Train Loss: 0.1151 | Val Loss: 0.2818 | ROC AUC: 0.9960\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "542a95b5226b47d2b21afec8ccd67c58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(Epoch 24) Train Loss: 0.1063 | Val Loss: 0.2572 | ROC AUC: 0.9960\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64a4a3b4f0c64278929a575a99ac867b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(Epoch 25) Train Loss: 0.1086 | Val Loss: 0.2311 | ROC AUC: 0.9990\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "178ba44783a04ff38910ba8f7a453f47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(Epoch 26) Train Loss: 0.1111 | Val Loss: 0.2303 | ROC AUC: 0.9990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=27` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇█</td></tr><tr><td>global_step</td><td>▁▁▂▂▃▃▄▄▅▅▆█</td></tr><tr><td>hard_loss</td><td>█▄▃▃▃▂▂▂▂▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>soft_loss</td><td>█▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_epoch</td><td>█▄▃▃▃▂▂▂▂▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▆▇▇▃▅▃▄▂▂▄▄▃▃▃▅▄▄▇▅▃▁█▄▂▂▃▃▂▄▂▁▂▂▄▂▁▂▃▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>val_loss</td><td>▇▅█▁▂▂▂▂▂▂▂▁▃▅▅▄▂▄▃▄▄▅▄▅▅▄▄</td></tr><tr><td>val_roc_auc</td><td>▃▇▁██▆▇██▅███▅▅▄█▆▆▇▇▇▇▇▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>26</td></tr><tr><td>global_step</td><td>969</td></tr><tr><td>hard_loss</td><td>0.114</td></tr><tr><td>soft_loss</td><td>0.10811</td></tr><tr><td>train_loss_epoch</td><td>0.11105</td></tr><tr><td>train_loss_step</td><td>0.16663</td></tr><tr><td>trainer/global_step</td><td>1538</td></tr><tr><td>val_loss</td><td>0.23028</td></tr><tr><td>val_roc_auc</td><td>0.99901</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Full Train</strong> at: <a href='https://wandb.ai/paraise-/PlantPathology2020/runs/pdbjs4kk' target=\"_blank\">https://wandb.ai/paraise-/PlantPathology2020/runs/pdbjs4kk</a><br> View project at: <a href='https://wandb.ai/paraise-/PlantPathology2020' target=\"_blank\">https://wandb.ai/paraise-/PlantPathology2020</a><br>Synced 6 W&B file(s), 12 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>wandb/run-20260119_085757-pdbjs4kk/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full training completed.\n",
      "CPU times: user 33min 25s, sys: 6min 10s, total: 39min 36s\n",
      "Wall time: 39min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "runner.run_full_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Inference & Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Full Inference (Model Soup) ===\n",
      "Averaging 5 checkpoints...\n",
      "Update BN stats (Full Data) ... \n",
      "BN updated model saved to: ../data/models/Dist_Model_ver1/final_soup_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Averaged model saved to: ../data/models/Dist_Model_ver1/final_soup_model.pth\n",
      "[Info] Full Training Mode\n",
      "Test: 1821\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0502a997f501450db893102a49ae2201",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 28s, sys: 35.2 s, total: 3min 3s\n",
      "Wall time: 3min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "final_preds_og = runner.run_inference(full=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV saved at ../data/models/Dist_Model_ver1/submission_og_Dist_Model_ver1.csv\n"
     ]
    }
   ],
   "source": [
    "result_sub = submission[['image_id']].copy()\n",
    "result_sub[hard_cols] = final_preds_og\n",
    "runner.backup_handler.save_file(result_sub, f'submission_og_{CFG.exp_name}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "N1w8ZC3_EJie",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>healthy</th>\n",
       "      <th>multiple_diseases</th>\n",
       "      <th>rust</th>\n",
       "      <th>scab</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Test_0</td>\n",
       "      <td>0.002275</td>\n",
       "      <td>0.012573</td>\n",
       "      <td>0.982910</td>\n",
       "      <td>0.002071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Test_1</td>\n",
       "      <td>0.001832</td>\n",
       "      <td>0.010574</td>\n",
       "      <td>0.984863</td>\n",
       "      <td>0.002899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Test_2</td>\n",
       "      <td>0.008202</td>\n",
       "      <td>0.009712</td>\n",
       "      <td>0.006954</td>\n",
       "      <td>0.975098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Test_3</td>\n",
       "      <td>0.980957</td>\n",
       "      <td>0.004520</td>\n",
       "      <td>0.009613</td>\n",
       "      <td>0.004715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Test_4</td>\n",
       "      <td>0.002312</td>\n",
       "      <td>0.009209</td>\n",
       "      <td>0.985352</td>\n",
       "      <td>0.003296</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  image_id   healthy  multiple_diseases      rust      scab\n",
       "0   Test_0  0.002275           0.012573  0.982910  0.002071\n",
       "1   Test_1  0.001832           0.010574  0.984863  0.002899\n",
       "2   Test_2  0.008202           0.009712  0.006954  0.975098\n",
       "3   Test_3  0.980957           0.004520  0.009613  0.004715\n",
       "4   Test_4  0.002312           0.009209  0.985352  0.003296"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(result_sub.head())"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 1026645,
     "sourceId": 18648,
     "sourceType": "competition"
    },
    {
     "datasetId": 9113217,
     "sourceId": 14395474,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31236,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
