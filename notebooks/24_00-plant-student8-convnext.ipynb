{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T23:38:26.167744Z",
     "iopub.status.busy": "2026-01-04T23:38:26.167583Z",
     "iopub.status.idle": "2026-01-04T23:38:37.851084Z",
     "shell.execute_reply": "2026-01-04T23:38:37.849879Z",
     "shell.execute_reply.started": "2026-01-04T23:38:26.167726Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import wandb\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "wandb_api_key = os.getenv(\"WANDB_API_KEY\")\n",
    "\n",
    "wandb.login(key=wandb_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-01-04T23:38:43.012538Z",
     "iopub.status.busy": "2026-01-04T23:38:43.012198Z",
     "iopub.status.idle": "2026-01-04T23:38:57.834656Z",
     "shell.execute_reply": "2026-01-04T23:38:57.833404Z",
     "shell.execute_reply.started": "2026-01-04T23:38:43.012501Z"
    },
    "id": "5qI9d5wIEJiY",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='pydantic')\n",
    "\n",
    "import gc\n",
    "import math\n",
    "import heapq\n",
    "import shutil\n",
    "import glob\n",
    "import random\n",
    "from types import SimpleNamespace\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import cv2\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.amp.autocast_mode import autocast\n",
    "from torch.amp.grad_scaler import GradScaler\n",
    "import timm\n",
    "import ttach as tta\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "cv2.setNumThreads(0)\n",
    "print(os.cpu_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    seed = 855\n",
    "    n_folds = 5\n",
    "    epochs = 25\n",
    "    virtual_epochs = 25\n",
    "    warmup_multiplier = 2\n",
    "    batch_size = 32\n",
    "    accum_iter = 1\n",
    "    num_workers = 4\n",
    "    persistent_workers=True\n",
    "    lr = 0.0001\n",
    "    weight_decay = 0.05\n",
    "    alpha = 0.5\n",
    "    T = 2.0\n",
    "    drop_path_rate = 0.2\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    project_name = 'PlantPathology2020'\n",
    "    exp_name = 'Student9_convnext_small'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed, deterministic=False):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed) # cpu\n",
    "    torch.cuda.manual_seed(seed) # gpu\n",
    "    torch.cuda.manual_seed_all(seed) # Î©ÄÌã∞ gpu\n",
    "    if deterministic:\n",
    "        os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.use_deterministic_algorithms(True)\n",
    "    else:\n",
    "        torch.backends.cudnn.deterministic = False\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        torch.use_deterministic_algorithms(False)\n",
    "\n",
    "\n",
    "set_seed(CFG.seed)\n",
    "\n",
    "device = CFG.device\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dWWDJu8cEJiZ"
   },
   "source": [
    "# 2. Data Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T23:41:44.010872Z",
     "iopub.status.busy": "2026-01-04T23:41:44.010535Z",
     "iopub.status.idle": "2026-01-04T23:41:44.033194Z",
     "shell.execute_reply": "2026-01-04T23:41:44.032435Z",
     "shell.execute_reply.started": "2026-01-04T23:41:44.010851Z"
    },
    "id": "lgnYMnwEEJia",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "img_dir = '../data/images/'\n",
    "\n",
    "hard_cols = ['healthy', 'multiple_diseases', 'rust', 'scab']\n",
    "soft_cols = ['healthy_pred', 'multiple_diseases_pred', 'rust_pred', 'scab_pred']\n",
    "\n",
    "train_df = pd.read_csv('../data/datasets/train_reborn_02.csv')\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "oof_df_01 = pd.read_csv('../data/datasets/oof_preds_Student5_EfficientNetB6_reborn.csv')\n",
    "oof_df_02 = pd.read_csv('../data/oof_preds_Student8_ResNest101e.csv')\n",
    "\n",
    "test_df = pd.read_csv('../data/test.csv')\n",
    "submission = pd.read_csv('../data/sample_submission.csv')\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T23:41:46.784275Z",
     "iopub.status.busy": "2026-01-04T23:41:46.783873Z",
     "iopub.status.idle": "2026-01-04T23:41:46.792296Z",
     "shell.execute_reply": "2026-01-04T23:41:46.791206Z",
     "shell.execute_reply.started": "2026-01-04T23:41:46.784245Z"
    },
    "id": "aY9purXqEJia",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_df['fold'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(oof_df_01.head())\n",
    "display(oof_df_02.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T23:41:48.130852Z",
     "iopub.status.busy": "2026-01-04T23:41:48.130427Z",
     "iopub.status.idle": "2026-01-04T23:41:48.143436Z",
     "shell.execute_reply": "2026-01-04T23:41:48.141861Z",
     "shell.execute_reply.started": "2026-01-04T23:41:48.130820Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "oof_df_01['image_id'] = train_df['image_id']\n",
    "oof_df_01 = oof_df_01[['image_id', 'healthy', 'multiple_diseases', 'rust', 'scab']]\n",
    "oof_df_01.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_df = oof_df_01.copy()\n",
    "oof_df[hard_cols] = oof_df_01[hard_cols] * 0.5 + oof_df_02[hard_cols] * 0.5\n",
    "oof_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T23:41:57.222504Z",
     "iopub.status.busy": "2026-01-04T23:41:57.222183Z",
     "iopub.status.idle": "2026-01-04T23:41:57.244473Z",
     "shell.execute_reply": "2026-01-04T23:41:57.243499Z",
     "shell.execute_reply.started": "2026-01-04T23:41:57.222477Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "oof_df.columns = ['image_id', 'healthy_pred', 'multiple_diseases_pred', 'rust_pred', 'scab_pred']\n",
    "train_df = train_df.merge(oof_df, on='image_id', how='left')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Load Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sv6kOrreM4ht"
   },
   "outputs": [],
   "source": [
    "all_images = {}\n",
    "# all_img_ids = np.concatenate([train_df['image_id'].values, test_df['image_id'].values])\n",
    "all_img_ids = np.concatenate([train_df['image_id'].tolist(), test_df['image_id'].tolist()])\n",
    "\n",
    "print(\"Loading all images into RAM once...\")\n",
    "for img_id in tqdm(all_img_ids, desc='Loading Images...' ,leave=False):\n",
    "    img = cv2.imread(img_dir + img_id + '.jpg')\n",
    "    img = cv2.resize(img, (650, 450))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img.setflags(write=False)\n",
    "    all_images[img_id] = img\n",
    "\n",
    "print(\"All Images on Ram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XZkc28p8EJib"
   },
   "source": [
    "## 2.3 Custom Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T15:39:27.318804Z",
     "iopub.status.busy": "2025-12-24T15:39:27.3184Z",
     "iopub.status.idle": "2025-12-24T15:39:27.327528Z",
     "shell.execute_reply": "2025-12-24T15:39:27.326254Z",
     "shell.execute_reply.started": "2025-12-24T15:39:27.318773Z"
    },
    "id": "8U3iknJbEJib",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, df, hard_cols=hard_cols, soft_cols=soft_cols, transform=None, is_test=False):\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        self.is_test = is_test\n",
    "        self.hard_cols = hard_cols\n",
    "        self.soft_cols = soft_cols\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.df.iloc[idx, 0]\n",
    "        image = all_images[img_id].copy()\n",
    "        # img_path = self.img_dir + img_id + '.jpg'\n",
    "        # image = cv2.imread(img_path)\n",
    "        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        # image = cv2.resize(image, (650, 450))\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image=image)['image']\n",
    "\n",
    "        if self.is_test:\n",
    "            return image\n",
    "        else:\n",
    "            soft_labels = self.df.iloc[idx][self.soft_cols].values.astype(np.float32)\n",
    "            hard_labels = self.df.iloc[idx][self.hard_cols].values.astype(np.float32)\n",
    "            return image, torch.tensor(soft_labels), torch.tensor(hard_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T2fNalwbEJib"
   },
   "source": [
    "## 2.4 Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T15:39:30.004086Z",
     "iopub.status.busy": "2025-12-24T15:39:30.003762Z",
     "iopub.status.idle": "2025-12-24T15:39:30.020758Z",
     "shell.execute_reply": "2025-12-24T15:39:30.019716Z",
     "shell.execute_reply.started": "2025-12-24T15:39:30.004059Z"
    },
    "id": "MYrlaHwhEJic",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# mean_fill_value = [103, 131, 82]\n",
    "\n",
    "transform_train = A.Compose([\n",
    "    A.OneOf([\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0, p=1.0),\n",
    "        A.RandomBrightnessContrast(brightness_limit=0, contrast_limit=0.2, p=1.0)\n",
    "    ], p=1.0),\n",
    "\n",
    "    A.OneOf([\n",
    "        A.MotionBlur(blur_limit=3, p=1.0),\n",
    "        A.MedianBlur(blur_limit=3, p=1.0),\n",
    "        A.GaussianBlur(blur_limit=3, p=1.0),\n",
    "    ], p=0.5),\n",
    "\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    \n",
    "    A.Affine(\n",
    "        scale=(0.8, 1.2),\n",
    "        translate_percent=0.2,\n",
    "        rotate=20,\n",
    "        interpolation=cv2.INTER_CUBIC, # Î≥¥Í∞Ñ\n",
    "        border_mode=cv2.BORDER_REFLECT_101, # ÌÖåÎëêÎ¶¨ Î∞òÏÇ¨ Ï±ÑÏö∞Í∏∞\n",
    "        p=1.0\n",
    "    ),\n",
    "\n",
    "    # A.CoarseDropout(\n",
    "    #     num_holes_range=(4, 8),       # min_holes=4, max_holes=8 ÎåÄÏ≤¥\n",
    "    #     hole_height_range=(8, 16),    # min_height=8, max_height=16 ÎåÄÏ≤¥\n",
    "    #     hole_width_range=(8, 16),     # min_width=8, max_width=16 ÎåÄÏ≤¥\n",
    "    #     fill=mean_fill_value,\n",
    "    #     p=0.5\n",
    "    # ),\n",
    "\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "transform_test = A.Compose([\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "transform_tta = tta.Compose(\n",
    "    [\n",
    "        tta.HorizontalFlip(),\n",
    "        tta.VerticalFlip(),\n",
    "        # tta.Rotate90(angles=[0, 90, 180, 270]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T15:39:31.611323Z",
     "iopub.status.busy": "2025-12-24T15:39:31.610921Z",
     "iopub.status.idle": "2025-12-24T15:39:31.618495Z",
     "shell.execute_reply": "2025-12-24T15:39:31.617535Z",
     "shell.execute_reply.started": "2025-12-24T15:39:31.611293Z"
    },
    "id": "vZi8NI_1EJic",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T15:39:32.188186Z",
     "iopub.status.busy": "2025-12-24T15:39:32.187777Z",
     "iopub.status.idle": "2025-12-24T15:39:32.194602Z",
     "shell.execute_reply": "2025-12-24T15:39:32.193603Z",
     "shell.execute_reply.started": "2025-12-24T15:39:32.188122Z"
    },
    "id": "yUqG6sYAEJic",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ÌÖåÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞ÏÖã\n",
    "batch_size = CFG.batch_size\n",
    "\n",
    "dataset_test = ImageDataset(test_df, transform=transform_test, is_test=True)\n",
    "loader_test = DataLoader(dataset_test, batch_size=batch_size*4,\n",
    "                         shuffle=False, worker_init_fn=seed_worker, generator=g, num_workers=CFG.num_workers, persistent_workers=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oohzJHhyEJic"
   },
   "source": [
    "# 3. Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T15:39:36.725853Z",
     "iopub.status.busy": "2025-12-24T15:39:36.725493Z",
     "iopub.status.idle": "2025-12-24T15:39:47.177407Z",
     "shell.execute_reply": "2025-12-24T15:39:47.17631Z",
     "shell.execute_reply.started": "2025-12-24T15:39:36.725822Z"
    },
    "id": "I-Y-n1CkEJid",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    model = timm.create_model(\n",
    "            'convnext_small.fb_in22k_ft_in1k',\n",
    "            pretrained=True,\n",
    "            drop_path_rate=CFG.drop_path_rate,\n",
    "            num_classes=4,\n",
    "        )\n",
    "    model = model.to(device)\n",
    "    return model\n",
    "\n",
    "model = get_model()\n",
    "config = timm.data.resolve_model_data_config(model)\n",
    "\n",
    "print(f\"Mean: {config['mean']}\")\n",
    "print(f\"Std: {config['std']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HgdyGRglni0k"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Training Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Metrics & Trackers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OdZxEQE7yhZy"
   },
   "outputs": [],
   "source": [
    "class AvgMeter:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "        self.avg = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "class MetricHandler:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.preds_list = []\n",
    "        self.actual_list = []\n",
    "\n",
    "    def update(self, preds, actual):\n",
    "        self.preds_list.extend(preds)\n",
    "        self.actual_list.extend(actual)\n",
    "\n",
    "    def compute_roc_auc(self):\n",
    "        return roc_auc_score(self.actual_list, self.preds_list)\n",
    "\n",
    "    def print_confusion_matrix(self):\n",
    "        y_true = np.argmax(np.array(self.actual_list), axis=1)\n",
    "        y_pred = np.argmax(np.array(self.preds_list), axis=1)\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "        class_total = cm.sum(axis=1)\n",
    "        class_correct = cm.diagonal()\n",
    "        class_acc = np.divide(class_correct, class_total, out=np.zeros_like(class_correct, dtype=float), where=class_total!=0)\n",
    "        class_names = ['Healthy', 'Multiple', 'Rust', 'Scab']\n",
    "\n",
    "        print('\\n' + '=' * 55)\n",
    "        print(f\"{'True \\\\ Pred':<12} | {'Healthy':^7} {'Multiple':^8} {'Rust':^7} {'Scab':^7}\")\n",
    "        print('-' * 55)\n",
    "        for i, label in enumerate(class_names):\n",
    "            print(f\"{label:<12} | {cm[i][0]:^7} {cm[i][1]:^8} {cm[i][2]:^7} {cm[i][3]:^7}\")\n",
    "        print('-' * 55)\n",
    "\n",
    "        print('üéØ Class-wise Accuracy:')\n",
    "        for i, name in enumerate(class_names):\n",
    "            acc_percent = class_acc[i] * 100\n",
    "            print(f' > {name:<9} : {acc_percent:6.2f}%  ({class_correct[i]:3d} / {class_total[i]:3d})')\n",
    "        print('=' * 55 + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelCheckpoint:\n",
    "    def __init__(self, output_dir, mode='max', top_k=3):\n",
    "        self.output_dir = output_dir\n",
    "        self.mode = mode\n",
    "        self.top_k = top_k\n",
    "        self.best_scores = [] # heap : [(compare_score, save_path), ...]\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "\n",
    "    def update(self, model, score, filename):\n",
    "        save_path = os.path.join(self.output_dir, filename)\n",
    "        compare_score = score if self.mode == 'max' else -score\n",
    "        save_flag = False\n",
    "        \n",
    "        if len(self.best_scores) < self.top_k:\n",
    "            save_flag = True\n",
    "        elif compare_score > self.best_scores[0][0]:\n",
    "            save_flag = True\n",
    "            _, remove_path = heapq.heappop(self.best_scores)\n",
    "            if os.path.exists(remove_path):\n",
    "                os.remove(remove_path)\n",
    "        \n",
    "        if save_flag:\n",
    "            heapq.heappush(self.best_scores, (compare_score, save_path))\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f'Top-{self.top_k} Model Saved : {filename} (score: {score:.4f})')\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "\n",
    "class BackupHandler:\n",
    "    def __init__(self, local_dir, backup_dir=None, active=True):\n",
    "        self.local_dir = local_dir\n",
    "        self.backup_dir = backup_dir\n",
    "        self.active = active and (backup_dir is not None)\n",
    "\n",
    "        if self.active and self.backup_dir is not None:\n",
    "            os.makedirs(self.backup_dir, exist_ok=True)\n",
    "            print(f'Backup Active : {self.local_dir} -> {self.backup_dir}')\n",
    "\n",
    "    def backup(self, filename):\n",
    "        if not self.active or self.backup_dir is None:\n",
    "            return\n",
    "\n",
    "        src_path = os.path.join(self.local_dir, filename)\n",
    "        dst_path = os.path.join(self.backup_dir, filename)\n",
    "        \n",
    "        if os.path.exists(src_path):\n",
    "            shutil.copy(src_path, dst_path)\n",
    "\n",
    "    def save_csv(self, df, filename):\n",
    "        local_path = os.path.join(self.local_dir, filename)\n",
    "        df.to_csv(local_path, index=False)\n",
    "        print(f'CSV saved at {local_path}')\n",
    "\n",
    "        if self.active and self.backup_dir is not None:\n",
    "            backup_path = os.path.join(self.backup_dir, filename)\n",
    "            df.to_csv(backup_path, index=False)\n",
    "            print(f'CSV saved at {backup_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Trainer Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, loader_train, loader_valid, fold, config, local_model_dir):\n",
    "        self.model = model\n",
    "        self.loader_train = loader_train\n",
    "        self.loader_valid = loader_valid\n",
    "        self.fold = fold\n",
    "        self.config = config\n",
    "        self.alpha = config.alpha\n",
    "        # self.T = config.T if hasattr(config, 'T') else 1.0\n",
    "        self.local_model_dir = local_model_dir\n",
    "        os.makedirs(self.local_model_dir, exist_ok=True)\n",
    "\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        self.loss_function = nn.CrossEntropyLoss()\n",
    "        self.scaler = GradScaler('cuda')\n",
    "        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.config.lr, weight_decay=self.config.weight_decay)\n",
    "        self.accum_iter = config.accum_iter\n",
    "\n",
    "        steps_per_epoch = math.ceil(len(loader_train.dataset) / self.config.batch_size)\n",
    "        total_steps = (steps_per_epoch // self.accum_iter) * config.virtual_epochs\n",
    "        warmup_steps = (steps_per_epoch // self.accum_iter) * config.warmup_multiplier if hasattr(config, 'warmup_multiplier') else 2\n",
    "        self.scheduler = get_cosine_schedule_with_warmup(\n",
    "            self.optimizer,\n",
    "            num_warmup_steps=warmup_steps,\n",
    "            num_training_steps=total_steps\n",
    "        )\n",
    "\n",
    "        self.metric_handler = MetricHandler()\n",
    "        # self.loss_checkpoint = ModelCheckpoint(output_dir=self.local_model_dir, mode='min')\n",
    "        self.score_checkpoint = ModelCheckpoint(output_dir=self.local_model_dir, mode='max')\n",
    "\n",
    "\n",
    "    def _train_one_epoch(self, epoch):\n",
    "        self.model.train()\n",
    "        epoch_train_loss = AvgMeter()\n",
    "        pbar = tqdm(self.loader_train, desc=f'Train {self.fold+1} Ep {epoch+1}', leave=False)\n",
    "        \n",
    "        T = self.config.T\n",
    "        self.optimizer.zero_grad()\n",
    "        # batch[0]=image, batch[1]=soft label, batch[2]=hard label\n",
    "        for i, batch in enumerate(pbar):\n",
    "            image = batch[0].to(self.device)\n",
    "            soft_labels = batch[1].to(self.device)\n",
    "            hard_labels = batch[2].to(self.device)\n",
    "            \n",
    "            if T > 1.0:\n",
    "                epsilon = 1e-6\n",
    "                logits_from_oof = torch.log(soft_labels + epsilon)\n",
    "                soft_labels = torch.softmax(logits_from_oof / T, dim=1)\n",
    "\n",
    "            label = self.alpha * hard_labels + (1 - self.alpha) * soft_labels\n",
    "            \n",
    "            with autocast('cuda'):\n",
    "                outputs = self.model(image)\n",
    "                logits = outputs / T if T > 1 else outputs\n",
    "                loss = self.loss_function(logits, label) \n",
    "                if T > 1.0:\n",
    "                    loss = loss * (T ** 2)\n",
    "                loss = loss / self.accum_iter\n",
    "\n",
    "            self.scaler.scale(loss).backward()\n",
    "            if (i + 1) % self.accum_iter == 0 or (i + 1) == len(self.loader_train):\n",
    "                scale_before = self.scaler.get_scale()\n",
    "                self.scaler.step(self.optimizer)\n",
    "                self.scaler.update()\n",
    "                scale_after = self.scaler.get_scale()\n",
    "                \n",
    "                if scale_after >= scale_before:\n",
    "                    self.scheduler.step()\n",
    "                    \n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "            epoch_train_loss.update(loss.item() * self.accum_iter, n=image.size(0))\n",
    "            pbar.set_postfix({'train_loss': epoch_train_loss.avg})\n",
    "\n",
    "        return epoch_train_loss.avg\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _val_one_epoch(self, epoch):\n",
    "        self.model.eval()\n",
    "        epoch_val_loss = AvgMeter()\n",
    "        epoch_val_conf = AvgMeter()\n",
    "        self.metric_handler.reset()\n",
    "\n",
    "        for image, soft_labels, hard_labels in tqdm(self.loader_valid, desc=f'Val {self.fold+1} Ep {epoch+1}', leave=False):\n",
    "            image = image.to(self.device)\n",
    "            hard_labels = hard_labels.to(self.device)\n",
    "            \n",
    "            with autocast('cuda'):\n",
    "                outputs = self.model(image)\n",
    "                loss = self.loss_function(outputs, hard_labels)\n",
    "            \n",
    "            probs = torch.softmax(outputs.cpu(), dim=1)\n",
    "            max_probs = probs.max(dim=1)[0]\n",
    "            epoch_val_conf.update(max_probs.mean().item(), n=image.size(0))\n",
    "\n",
    "            epoch_val_loss.update(loss.item(), n=image.size(0))\n",
    "            preds = probs.numpy()\n",
    "            # preds = torch.softmax(outputs.cpu(), dim=1).numpy()\n",
    "            self.metric_handler.update(preds, hard_labels.cpu().numpy())\n",
    "\n",
    "        val_loss = epoch_val_loss.avg\n",
    "        roc_auc = self.metric_handler.compute_roc_auc()\n",
    "        val_conf = epoch_val_conf.avg\n",
    "\n",
    "        return val_loss, roc_auc, val_conf\n",
    "\n",
    "    def fit(self, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            set_seed(self.config.seed + epoch)\n",
    "\n",
    "            train_loss = self._train_one_epoch(epoch)\n",
    "            val_loss, roc_auc, val_conf = self._val_one_epoch(epoch)\n",
    "\n",
    "            print(f'EPOCH [{epoch+1}/{epochs}] || TRAIN LOSS : {train_loss:.4f} || VAL LOSS : {val_loss:.4f} / ROC AUC : {roc_auc:.4f} | Avg Conf: {val_conf:.4f} ')\n",
    "\n",
    "            current_lr = self.optimizer.param_groups[0]['lr']\n",
    "            wandb.log({\n",
    "                \"train/loss\": train_loss,\n",
    "                \"val/loss\": val_loss,\n",
    "                \"val/auc\": roc_auc,\n",
    "                \"learning_rate\": current_lr,\n",
    "                \"epoch\": epoch + 1\n",
    "            })\n",
    "\n",
    "            # best_loss_name = f'best_loss_model_{self.fold+1}_loss_{val_loss}.pth'\n",
    "            best_score_name = f'best_score_model_{self.fold+1}_ep{epoch+1}_roc_{roc_auc:.4f}.pth'\n",
    "\n",
    "            # self.loss_checkpoint.update(self.model, val_loss, best_loss_name)\n",
    "\n",
    "            if self.score_checkpoint.update(self.model, roc_auc, best_score_name):\n",
    "                self.metric_handler.print_confusion_matrix()\n",
    "                if wandb.run is not None:\n",
    "                    wandb.run.summary[\"best_auc\"] = max(self.score_checkpoint.best_scores)[0]\n",
    "\n",
    "        return max(self.score_checkpoint.best_scores)[0],# -max(self.loss_checkpoint.best_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor:\n",
    "    def __init__(self, device, model_arch=None,  tta_transform=None):\n",
    "        self.model_arch = model_arch\n",
    "        self.device = device\n",
    "        self.tta_transform = tta_transform\n",
    "        if self.tta_transform and (model_arch is not None):\n",
    "            self.model = tta.ClassificationTTAWrapper(self.model_arch, self.tta_transform, merge_mode='mean')\n",
    "        \n",
    "    def load_weights(self, weight_path):\n",
    "        state_dict = torch.load(weight_path, map_location=self.device)\n",
    "        self.model_arch.load_state_dict(state_dict)\n",
    "        self.model = self.model_arch.to(self.device)\n",
    "        if self.tta_transform:\n",
    "            self.model = tta.ClassificationTTAWrapper(self.model, self.tta_transform, merge_mode='mean')\n",
    "        \n",
    "        self.model.eval()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict(self, loader):\n",
    "        preds_list = []\n",
    "        for batch in tqdm(loader, leave=False):\n",
    "            if isinstance(batch, (list, tuple)):\n",
    "                image = batch[0]\n",
    "            else:\n",
    "                image = batch\n",
    "            \n",
    "            image = image.to(self.device)\n",
    "\n",
    "            with autocast('cuda'):\n",
    "                outputs = self.model(image)\n",
    "\n",
    "            preds = torch.softmax(outputs.cpu(), dim=1).numpy()\n",
    "            preds_list.append(preds)\n",
    "\n",
    "        final_preds = np.concatenate(preds_list, axis=0)\n",
    "\n",
    "        return final_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Experiment Ochestrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentRunner:\n",
    "    def __init__(self, config, train_df, test_df):\n",
    "        self.config = config\n",
    "        self.train_df = train_df\n",
    "        self.test_df = test_df\n",
    "        self.paths = self._setup_env()\n",
    "        self.backup_handler = BackupHandler(local_dir=self.paths.local_path , backup_dir=self.paths.drive_path, active=False)\n",
    "\n",
    "    def _setup_env(self):\n",
    "        is_kaggle = os.path.exists('/kaggle/') \n",
    "        is_colab = os.path.exists('/content/drive/Mydrive') and not is_kaggle\n",
    "\n",
    "        if is_kaggle:\n",
    "            print(\"Environment: Kaggle\")\n",
    "            drive_path = None\n",
    "            local_path = '/kaggle/working/'\n",
    "        elif is_colab:\n",
    "            print(\"Environment: Google Colab\")\n",
    "            drive_path = f'/content/drive/MyDrive/Kaggle_Save/{CFG.exp_name}/'\n",
    "            local_path = '/content/models/'\n",
    "        else:\n",
    "            print(\"Environment: Local\")\n",
    "            drive_path = None\n",
    "            local_path = f'../data/models/{CFG.exp_name}/'\n",
    "        \n",
    "        print(f\"Save Path: {local_path}\")\n",
    "        return SimpleNamespace(local_path=local_path, drive_path=drive_path)\n",
    "\n",
    "    def _run_fold(self, fold, loader_train, loader_valid):\n",
    "        model = get_model()      \n",
    "        trainer = Trainer(model, loader_train, loader_valid, fold, self.config, self.paths.local_path)\n",
    "        best_score = trainer.fit(self.config.epochs)\n",
    "        return trainer, best_score\n",
    "\n",
    "    def run_experiment(self):\n",
    "        for fold in range(self.config.n_folds):\n",
    "            print('='*30, f'FOLD {fold+1}', '='*30)\n",
    "\n",
    "            # wanb run Ï¥àÍ∏∞Ìôî\n",
    "            if wandb.run is not None:\n",
    "                wandb.finish()\n",
    "\n",
    "            wandb.init(\n",
    "                project=self.config.project_name,\n",
    "                group=self.config.exp_name,\n",
    "                name=f\"Fold_{fold+1}\",\n",
    "                job_type=\"train\",\n",
    "                config={k: v for k, v in self.config.__dict__.items() if not k.startswith('__')}\n",
    "            )\n",
    "            \n",
    "            # Îç∞Ïù¥ÌÑ∞ Î°úÎçî Ï§ÄÎπÑ\n",
    "            train = self.train_df[self.train_df['fold']!=fold].reset_index(drop=True).copy()\n",
    "            valid = self.train_df[self.train_df['fold']==fold].copy()\n",
    "\n",
    "            valid_indices = valid.index.values\n",
    "            valid = valid.reset_index(drop=True)\n",
    "\n",
    "            print(f'train size : {len(train)}')\n",
    "            print(f'valid size : {len(valid)}')\n",
    "\n",
    "            dataset_train = ImageDataset(train, transform=transform_train)\n",
    "            dataset_valid = ImageDataset(valid, transform=transform_test)\n",
    "\n",
    "            loader_train = DataLoader(dataset_train, batch_size=self.config.batch_size, shuffle=True,\n",
    "                                    worker_init_fn=seed_worker, generator=g, num_workers=self.config.num_workers, persistent_workers=True, pin_memory=True)\n",
    "            loader_valid = DataLoader(dataset_valid, batch_size=self.config.batch_size*4, shuffle=False,\n",
    "                                    worker_init_fn=seed_worker, generator=g, num_workers=self.config.num_workers, persistent_workers=True, pin_memory=True)\n",
    "\n",
    "\n",
    "            trainer, best_score = self._run_fold(fold=fold, loader_train=loader_train, loader_valid=loader_valid)\n",
    "\n",
    "            # backup_handler.backup(trainer.best_loss_name)\n",
    "            # backup_handler.backup(trainer.best_score_name)\n",
    "\n",
    "            # Î°úÍπÖ Ï¢ÖÎ£å\n",
    "            wandb.finish()\n",
    "            \n",
    "            # Î©îÎ™®Î¶¨ Ï†ïÎ¶¨\n",
    "            del loader_train, loader_valid, trainer, dataset_train, dataset_valid\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            \n",
    "    def _load_averaged_model(self, fold, model_arch):\n",
    "        save_path = os.path.join(self.paths.local_path, f'best_score_model_{fold+1}.pth')\n",
    "        model = model_arch.to(self.config.device)\n",
    "\n",
    "        if os.path.exists(save_path):\n",
    "            print(f\"‚úÖ Found existing averaged model for Fold {fold+1}. Loading directly...\")\n",
    "            state_dict = torch.load(save_path, map_location=self.config.device)\n",
    "            model.load_state_dict(state_dict)\n",
    "        else:\n",
    "            print(f'Merging Top-K Models for Fold {fold+1} ...')\n",
    "            score_pattern = os.path.join(self.paths.local_path, f'best_score_model_{fold+1}_*.pth')\n",
    "            score_files = glob.glob(score_pattern)\n",
    "            print(f'Found {len(score_files)} score models : {[os.path.basename(f) for f in score_files]}')\n",
    "            \n",
    "            avg_state_dict = torch.load(score_files[0], map_location=self.config.device)\n",
    "            if len(score_files) > 1:\n",
    "                for path in score_files[1:]:\n",
    "                    state_dict = torch.load(path, map_location=self.config.device)\n",
    "                    for key in avg_state_dict:\n",
    "                        avg_state_dict[key] += state_dict[key]\n",
    "                for key in avg_state_dict:\n",
    "                    if avg_state_dict[key].is_floating_point():\n",
    "                        avg_state_dict[key] = avg_state_dict[key] / len(score_files)\n",
    "                    else:\n",
    "                        avg_state_dict[key] = avg_state_dict[key] // len(score_files)\n",
    "            \n",
    "            model = model_arch.to(self.config.device)\n",
    "            model.load_state_dict(avg_state_dict)\n",
    "            \n",
    "            for remove_path in score_files:\n",
    "                if os.path.exists(remove_path):\n",
    "                    os.remove(remove_path)\n",
    "            \n",
    "            save_path = os.path.join(self.paths.local_path, f'best_score_model_{fold+1}.pth')\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print('Save Avg Model : ', save_path)\n",
    "        \n",
    "        print('Update BN stats ... ')\n",
    "        model.train()\n",
    "        train_subset = self.train_df[self.train_df['fold'] != fold]\n",
    "        dataset_bn = ImageDataset(train_subset, transform=transform_test)\n",
    "        loader_bn = DataLoader(dataset=dataset_bn, batch_size=self.config.batch_size, shuffle=False,\n",
    "                                worker_init_fn=seed_worker, generator=g, num_workers=self.config.num_workers, persistent_workers=True, pin_memory=True)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(loader_bn, desc=f'Update BN stats for Fold {fold+1}', leave=False):\n",
    "                model(batch[0].to(self.config.device))\n",
    "                \n",
    "        model.eval()\n",
    "        return model\n",
    "        \n",
    "    def run_inference(self):\n",
    "        oof_preds = np.zeros((len(self.train_df), 4))\n",
    "        final_preds = np.zeros((len(self.test_df), 4))\n",
    "\n",
    "        for fold in range(self.config.n_folds):\n",
    "            print(f\"=== Inference Fold {fold+1} ===\")\n",
    "            avg_model = self._load_averaged_model(fold, get_model())\n",
    "            \n",
    "            predictor = Predictor(device=self.config.device, model_arch=avg_model, tta_transform=transform_tta)\n",
    "            # sub_predictor = Predictor(model_arch=get_model(), device=device, tta_transform=transform_tta)\n",
    "\n",
    "            valid = self.train_df[self.train_df['fold']==fold].copy()\n",
    "            valid_indices = valid.index.values\n",
    "            dataset_valid = ImageDataset(valid, transform=transform_test)\n",
    "            loader_valid = DataLoader(dataset_valid, batch_size=self.config.batch_size*4, shuffle=False,\n",
    "                                    worker_init_fn=seed_worker, generator=g, num_workers=self.config.num_workers, persistent_workers=True, pin_memory=True)\n",
    "\n",
    "            # oof_predictor.load_weights(score_model_path)\n",
    "            print(f\"OOF Inference ... \")\n",
    "            oof_temp = predictor.predict(loader_valid)\n",
    "            oof_preds[valid_indices] = oof_temp\n",
    "\n",
    "            # sub_predictor.load_weights(score_model_path)\n",
    "            print(f\"Test Inference ... \")\n",
    "            sub_temp = predictor.predict(loader_test)\n",
    "            final_preds += (sub_temp / self.config.n_folds)\n",
    "\n",
    "            del predictor, loader_valid, dataset_valid\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "        metric_handler = MetricHandler()\n",
    "        metric_handler.update(oof_preds, self.train_df[hard_cols].values)\n",
    "        oof_roc = metric_handler.compute_roc_auc()\n",
    "        print(f'OOF ROC AUC : {oof_roc:.4f}')\n",
    "\n",
    "        return oof_preds, final_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Training Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = ExperimentRunner(config=CFG, train_df=train_df, test_df=test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "runner.run_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Inference & Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_list = glob.glob(runner.paths.local_path + \"/*.pth\")\n",
    "print([m.split('/')[-1] for m in models_list ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "oof_preds, final_preds = runner.run_inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_oof = train_df[['image_id']].copy()\n",
    "result_oof[hard_cols] = oof_preds\n",
    "runner.backup_handler.save_csv(result_oof, f'oof_preds_{CFG.exp_name}.csv')\n",
    "\n",
    "result_sub = submission[['image_id']].copy()\n",
    "result_sub[hard_cols] = final_preds\n",
    "runner.backup_handler.save_csv(result_sub, f'submission_{CFG.exp_name}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N1w8ZC3_EJie",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "display(result_oof.head())\n",
    "display(result_sub.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x03FZaQmuxX_"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 1026645,
     "sourceId": 18648,
     "sourceType": "competition"
    },
    {
     "datasetId": 9113217,
     "sourceId": 14395474,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31236,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
