{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T23:38:26.167744Z",
     "iopub.status.busy": "2026-01-04T23:38:26.167583Z",
     "iopub.status.idle": "2026-01-04T23:38:37.851084Z",
     "shell.execute_reply": "2026-01-04T23:38:37.849879Z",
     "shell.execute_reply.started": "2026-01-04T23:38:26.167726Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /teamspace/studios/this_studio/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mparaise\u001b[0m (\u001b[33mparaise-\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import wandb\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "wandb_api_key = os.getenv(\"WANDB_API_KEY\")\n",
    "\n",
    "wandb.login(key=wandb_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-01-04T23:38:43.012538Z",
     "iopub.status.busy": "2026-01-04T23:38:43.012198Z",
     "iopub.status.idle": "2026-01-04T23:38:57.834656Z",
     "shell.execute_reply": "2026-01-04T23:38:57.833404Z",
     "shell.execute_reply.started": "2026-01-04T23:38:43.012501Z"
    },
    "id": "5qI9d5wIEJiY",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='pydantic')\n",
    "\n",
    "import gc\n",
    "import math\n",
    "import shutil\n",
    "import glob\n",
    "import random\n",
    "from types import SimpleNamespace\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import cv2\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import torch\n",
    "from torch.optim.swa_utils import update_bn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from lightning.pytorch.callbacks import TQDMProgressBar, Callback\n",
    "\n",
    "from torchmetrics import ConfusionMatrix, AUROC\n",
    "import timm\n",
    "import ttach as tta\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cv2.setNumThreads(0)\n",
    "print(os.cpu_count())\n",
    "torch.set_float32_matmul_precision('high') # L4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.22'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timm.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    model_arch = 'resnest101e'\n",
    "    is_bn = True\n",
    "    seed = 938\n",
    "    lr = 0.0001\n",
    "    weight_decay = 0.05\n",
    "    alpha = 0.7\n",
    "    weak_alpha = 0.3\n",
    "    strong_alpha = 0.7\n",
    "    T = 1.25\n",
    "    drop_path_rate = 0.2\n",
    "    top_k = 3\n",
    "    n_folds = 5\n",
    "    epochs = 24\n",
    "    batch_size = 32\n",
    "    accum_iter = 1\n",
    "    num_workers = 4\n",
    "    persistent_workers=True\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    project_name = 'PlantPathology2020'\n",
    "    exp_name = 's14_resnest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed, deterministic=False):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed) # cpu\n",
    "    torch.cuda.manual_seed(seed) # gpu\n",
    "    torch.cuda.manual_seed_all(seed) # 멀티 gpu\n",
    "    if deterministic:\n",
    "        os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.use_deterministic_algorithms(True)\n",
    "    else:\n",
    "        torch.backends.cudnn.deterministic = False\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        torch.use_deterministic_algorithms(False)\n",
    "\n",
    "set_seed(CFG.seed)\n",
    "device = CFG.device\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipynbname\n",
    "current_file = ipynbname.name() + \".ipynb\"\n",
    "print(current_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dWWDJu8cEJiZ"
   },
   "source": [
    "# 2. Data Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data & Knowledge Source\n",
    "- Teacher Logits:\n",
    "\n",
    "    - Teacher 1: ConvNeXt-Small\n",
    "\n",
    "    - Teacher 2: ConvNeXt-Small variant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModule:\n",
    "    def __init__(self, data_dir, logit_path1, logit_path2):\n",
    "        self.data_dir = data_dir\n",
    "        self.img_dir = self.data_dir + 'images/'\n",
    "        self.logit_path1 = logit_path1\n",
    "        self.logit_path2 = logit_path2\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        train_df = pd.read_csv(self.data_dir + 'datasets/train_reborn_02.csv')\n",
    "        train_df = train_df.reset_index(drop=True)\n",
    "        test_df = pd.read_csv(self.data_dir + 'test.csv')\n",
    "        submission = pd.read_csv(self.data_dir + 'sample_submission.csv')\n",
    "        teacher_logit1 = np.load(self.logit_path1)\n",
    "        teacher_logit2 = np.load(self.logit_path2)\n",
    "        teacher_logit = teacher_logit1* 0.5 + teacher_logit2 * 0.5\n",
    "        print(f\"Logit1 Range: {teacher_logit1.min():.2f} ~ {teacher_logit1.max():.2f}\")\n",
    "        print(f\"Logit2 Range: {teacher_logit2.min():.2f} ~ {teacher_logit2.max():.2f}\")\n",
    "        return train_df, teacher_logit, test_df, submission\n",
    "\n",
    "    \n",
    "train_df, teacher_logit, test_df, submission = DataModule(data_dir='../data/',\n",
    " logit_path1='../data/models/s10_convnext_small_T_scheduler/oof_ogit_s10_convnext_small_T_scheduler.npy',\n",
    " logit_path2='../data/models/s12_conv_teacher_s10_v2/oof_logit_s12_conv_teacher_s10_v2.npy').prepare_data()\n",
    "\n",
    "hard_cols = ['healthy', 'multiple_diseases', 'rust', 'scab']\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Load Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images = {}\n",
    "all_img_ids = np.concatenate([train_df['image_id'].tolist(), test_df['image_id'].tolist()])\n",
    "\n",
    "print(\"Loading all images into RAM once...\")\n",
    "for img_id in tqdm(all_img_ids, desc='Loading Images...' ,leave=False):\n",
    "    img = cv2.imread('../data/images/' + img_id + '.jpg')\n",
    "    img = cv2.resize(img, (650, 450))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img.setflags(write=False)\n",
    "    all_images[img_id] = img\n",
    "\n",
    "print(\"All Images on Ram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, df, hard_cols=hard_cols, teacher_logit=None, transform=None, is_test=False):\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        self.is_test = is_test\n",
    "        self.hard_cols = hard_cols\n",
    "        self.teacher_logit = teacher_logit\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.df.iloc[idx, 0]\n",
    "        image = all_images[img_id].copy()\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image=image)['image']\n",
    "\n",
    "        if self.is_test:\n",
    "            return image\n",
    "        else:\n",
    "            hard_labels = self.df.iloc[idx][self.hard_cols].values.astype(np.float32)\n",
    "            oof_logit = self.teacher_logit[idx]\n",
    "            return image, torch.tensor(oof_logit), torch.tensor(hard_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlantDataModule(pl.LightningDataModule):\n",
    "    \"\"\"\n",
    "    데이터 로딩, 전처리 및 학습/검증 세트 분할을 관리하는 클래스입니다.\n",
    "    \n",
    "    K-Fold 인덱스에 따라 데이터를 학습용과 검증용으로 분리하며, \n",
    "    stage 인자에 따라 불필요한 데이터 로딩을 방지하여 메모리 효율성을 최적화합니다. \n",
    "    이미지 증강(Augmentation) 로직을 내부적으로 포함하여 데이터와 모델 사이의 인터페이스를 명확히 정의합니다.\n",
    "    \"\"\"\n",
    "    def __init__(self, train_df, teacher_logit, test_df, cfg, fold_idx, inference_mode=False):\n",
    "        super().__init__()\n",
    "        self.train_df = train_df\n",
    "        self.teacher_logit = teacher_logit\n",
    "        self.test_df = test_df\n",
    "        self.cfg = cfg\n",
    "        self.fold_idx = fold_idx\n",
    "        self.inference_mode = inference_mode\n",
    "\n",
    "        self.transform_train = A.Compose([\n",
    "            A.OneOf([\n",
    "                A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0, p=1.0),\n",
    "                A.RandomBrightnessContrast(brightness_limit=0, contrast_limit=0.2, p=1.0)\n",
    "            ], p=1.0),\n",
    "\n",
    "            A.OneOf([\n",
    "                A.MotionBlur(blur_limit=5, p=1.0),\n",
    "                A.MedianBlur(blur_limit=5, p=1.0),\n",
    "                A.GaussianBlur(blur_limit=(3, 5), p=1.0),\n",
    "            ], p=0.5),\n",
    "            \n",
    "            A.OneOf([\n",
    "                A.Affine(\n",
    "                    scale=(0.8, 1.2),       \n",
    "                    translate_percent=0.2,  \n",
    "                    rotate=45,              # 회전 각도 확대\n",
    "                    shear=20,               # 전단 변환(기울기) 추가\n",
    "                    interpolation=cv2.INTER_CUBIC,\n",
    "                    border_mode=cv2.BORDER_REFLECT_101, \n",
    "                    p=1.0\n",
    "                ),\n",
    "                A.Perspective(scale=(0.05, 0.1), p=1.0), # 원근 변환\n",
    "            ], p=0.8),\n",
    "            \n",
    "            # 질감/노이즈\n",
    "            A.OneOf([\n",
    "                A.ISONoise(p=1.0),\n",
    "                A.GaussNoise(p=1.0),\n",
    "            ], p=0.3),\n",
    "\n",
    "            A.CoarseDropout(\n",
    "                num_holes_range=(8, 16),\n",
    "                hole_height_range=(8, 16),\n",
    "                hole_width_range=(8, 16),\n",
    "                fill=[103, 131, 82],\n",
    "                p=0.5\n",
    "            ),\n",
    "\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "\n",
    "            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "\n",
    "        self.transform_test = A.Compose([\n",
    "            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        if stage == 'fit' or stage is None:\n",
    "            train_mask = (self.train_df['fold']!=self.fold_idx).values\n",
    "            val_mask = (self.train_df['fold']==self.fold_idx).values\n",
    "\n",
    "            self.train = self.train_df[train_mask].reset_index(drop=True)\n",
    "            self.valid = self.train_df[val_mask].reset_index(drop=True)\n",
    "            train_logit = self.teacher_logit[train_mask]\n",
    "            val_logit = self.teacher_logit[val_mask]\n",
    "\n",
    "            self.dataset_train = ImageDataset(self.train, teacher_logit=train_logit, transform=self.transform_train)\n",
    "            self.dataset_valid = ImageDataset(self.valid, teacher_logit=val_logit, transform=self.transform_test)\n",
    "            print(f'[Fit] Train: {len(self.train)}, Valid: {len(self.valid)}')\n",
    "\n",
    "        elif stage == 'test':\n",
    "            val_mask = (self.train_df['fold'] == self.fold_idx).values\n",
    "            self.valid = self.train_df[val_mask].reset_index(drop=True)\n",
    "            val_logit = self.teacher_logit[val_mask]\n",
    "            \n",
    "            self.dataset_valid = ImageDataset(self.valid, teacher_logit=val_logit, transform=self.transform_test)\n",
    "            self.dataset_test = ImageDataset(self.test_df, transform=self.transform_test, is_test=True)\n",
    "            print(f'[Test] Valid(OOF): {len(self.valid)}, Test: {len(self.test_df)}')\n",
    "\n",
    "        elif stage == 'predict':\n",
    "            self.dataset_test = ImageDataset(self.test_df, transform=self.transform_test, is_test=True)\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        loader_train = DataLoader(self.dataset_train, batch_size=self.cfg.batch_size, shuffle=True,\n",
    "                                worker_init_fn=seed_worker, generator=g, num_workers=self.cfg.num_workers, \n",
    "                                persistent_workers=True, pin_memory=True)\n",
    "        return loader_train\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        user_persistent = not self.inference_mode\n",
    "        loader_valid = DataLoader(self.dataset_valid, batch_size=self.cfg.batch_size*4, shuffle=False,\n",
    "                                worker_init_fn=seed_worker, generator=g, num_workers=self.cfg.num_workers, \n",
    "                                persistent_workers=user_persistent, pin_memory=True)\n",
    "        return loader_valid\n",
    "    \n",
    "    def predict_dataloader(self):\n",
    "        loader_test = DataLoader(self.dataset_test, batch_size=self.cfg.batch_size*4, shuffle=False,\n",
    "                                worker_init_fn=seed_worker, generator=g, num_workers=self.cfg.num_workers, \n",
    "                                persistent_workers=False, pin_memory=True)\n",
    "        return loader_test\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return self.predict_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oohzJHhyEJic"
   },
   "source": [
    "# 3. Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FoldAlphaCallback(Callback):\n",
    "    \"\"\"\n",
    "    폴드별로 Knowledge Distillation의 Alpha 값을 다르게 조정하는 콜백\n",
    "    - 성능이 낮은 폴드: GT 비중(Alpha)을 높여 Teacher 의존도 낮춤\n",
    "    - 성능이 높은 폴드: Teacher 비중(1-Alpha)을 높여 정규화 효과 극대화\n",
    "    \"\"\"\n",
    "    def __init__(self, current_fold, weak_folds=[0, 1], weak_alpha=0.8, strong_alpha=0.5):\n",
    "        super().__init__()\n",
    "        self.current_fold = current_fold\n",
    "        self.weak_folds = weak_folds\n",
    "        self.weak_alpha = weak_alpha\n",
    "        self.strong_alpha = strong_alpha\n",
    "\n",
    "    def on_train_start(self, trainer, pl_module):\n",
    "        # 학습 시작 시점에 폴드에 맞는 Alpha 설정\n",
    "        if self.current_fold in self.weak_folds:\n",
    "            pl_module.current_alpha = self.weak_alpha\n",
    "            strategy = \"GT Focus (Weak Fold)\"\n",
    "        else:\n",
    "            pl_module.current_alpha = self.strong_alpha\n",
    "            strategy = \"Regularization (Strong Fold)\"\n",
    "            \n",
    "        # 로그 출력 (한 번만)\n",
    "        if trainer.global_rank == 0:\n",
    "            print(f\"\\n[FoldAlphaCallback] Fold {self.current_fold+1}: \"\n",
    "                  f\"Alpha set to {pl_module.current_alpha} ({strategy})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlantDiseaseModule(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    모델의 순전파, 손실 함수 계산, 최적화 알고리즘 및 메트릭 측정을 캡슐화합니다.\n",
    "    특히 훈련 단계에서는 Soft Label Mixing(Knowledge Distillation 원리 적용)을 통해 \n",
    "    라벨 노이즈에 대한 강건성을 확보하며, 추론 단계에서는 TTA(Test Time Augmentation)를 \n",
    "    통합하여 예측의 불확실성을 줄이고 일반화 성능을 향상시킵니다.\n",
    "    \"\"\"\n",
    "    def __init__(self, config, steps_per_epoch=None):\n",
    "        super().__init__()\n",
    "        if isinstance(config, type):\n",
    "            config = {k: v for k, v in config.__dict__.items() if not k.startswith('__')}\n",
    "        self.save_hyperparameters(config)\n",
    "        self.current_alpha = self.hparams.alpha\n",
    "        self.model = timm.create_model(\n",
    "            self.hparams.model_arch,\n",
    "            pretrained=True,\n",
    "            drop_path_rate=self.hparams.drop_path_rate,\n",
    "            num_classes=4\n",
    "            )\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "        # TTA\n",
    "        self.tta_transforms = tta.Compose([\n",
    "                tta.HorizontalFlip(),\n",
    "                tta.VerticalFlip(),\n",
    "            ])\n",
    "        \n",
    "        # metrics\n",
    "        self.valid_auc = AUROC(task='multiclass', num_classes=4)\n",
    "        self.valid_cm = ConfusionMatrix(task='multiclass', num_classes=4)\n",
    "        self.best_score = 0.0\n",
    "\n",
    "        self.top_k_scores = []  # (score, epoch) 튜플을 저장할 리스트\n",
    "        self.top_k = self.hparams.top_k\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.hparams.lr, weight_decay=self.hparams.weight_decay)\n",
    "        \n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=8, T_mult=2, eta_min=1e-6)\n",
    "        \n",
    "        scheduler_config = {\n",
    "            'scheduler' : scheduler,\n",
    "            'interval' : 'epoch',\n",
    "            'frequency' : 1\n",
    "        }\n",
    "        \n",
    "        return [optimizer], [scheduler_config]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        T = self.hparams.T\n",
    "        alpha = self.current_alpha\n",
    "\n",
    "        image, logit_from_oof, hard_labels = batch\n",
    "        outputs = self.model(image)\n",
    "\n",
    "        # 하드라벨 손실\n",
    "        loss_hard = self.criterion(outputs, hard_labels)\n",
    "\n",
    "        # Teacher\n",
    "        teacher_probs = torch.softmax(logit_from_oof / T, dim=1)\n",
    "        # Student\n",
    "        student_log_probs = torch.nn.functional.log_softmax(outputs / T, dim=1)\n",
    "\n",
    "        kl_loss = torch.nn.functional.kl_div(\n",
    "            student_log_probs, \n",
    "            teacher_probs, \n",
    "            reduction='batchmean'\n",
    "        )\n",
    "        loss_soft = kl_loss * (T**2)\n",
    "        # total loss\n",
    "        loss = alpha * loss_hard + (1 - alpha) * loss_soft\n",
    "        # log\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('hard_loss', loss_hard, on_step=False, on_epoch=True, logger=True)\n",
    "        self.log('soft_loss', loss_soft, on_step=False, on_epoch=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        image, _, hard_labels = batch\n",
    "        outputs = self.model(image)\n",
    "        loss = self.criterion(outputs, hard_labels)\n",
    "        \n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        targets = torch.argmax(hard_labels, dim=1)\n",
    "\n",
    "        self.valid_cm(preds, targets)\n",
    "        self.valid_auc(probs, targets)\n",
    "        \n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('val_roc_auc', self.valid_auc, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        if self.trainer.sanity_checking:\n",
    "            return\n",
    "                  \n",
    "        score = self.trainer.callback_metrics.get('val_roc_auc')\n",
    "        current_epoch = self.current_epoch\n",
    "        \n",
    "        if score is not None:\n",
    "            current_score = score.item()            \n",
    "            self.top_k_scores.append((current_score, current_epoch))\n",
    "            self.top_k_scores.sort(key=lambda x: x[0], reverse=True)\n",
    "            self.top_k_scores = self.top_k_scores[:self.top_k]\n",
    "            is_in_top_k = (current_score, current_epoch) in self.top_k_scores\n",
    "            \n",
    "            if is_in_top_k and isinstance(self.logger, WandbLogger):\n",
    "                rank = self.top_k_scores.index((current_score, current_epoch)) + 1\n",
    "                top_k_str = \", \".join([f\"(Ep {e}: {s:.4f})\" for s, e in self.top_k_scores])\n",
    "                self.print(f\"Current Top-{self.top_k}: {top_k_str}\")\n",
    "                \n",
    "                cm = self.valid_cm.compute().cpu().numpy()\n",
    "                columns = ['Healthy', 'Multiple', 'Rust', 'Scab']\n",
    "                plt.figure(figsize=(10, 8))\n",
    "                sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                            xticklabels=columns, yticklabels=columns,\n",
    "                            annot_kws={\"size\": 14})\n",
    "                            \n",
    "                plt.ylabel('True Label', fontsize=12)\n",
    "                plt.xlabel('Predicted Label', fontsize=12)\n",
    "                plt.title(f'Confusion Matrix (Epoch {current_epoch})', fontsize=14)\n",
    "             \n",
    "                self.logger.experiment.log({\n",
    "                    \"val/confusion_matrix\": wandb.Image(plt, caption=f\"Epoch {current_epoch}\"),\n",
    "                    \"global_step\": self.global_step\n",
    "                })\n",
    "                plt.close()\n",
    "\n",
    "        self.valid_cm.reset()\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        score = self.trainer.callback_metrics.get('val_roc_auc')\n",
    "        train_loss = self.trainer.callback_metrics.get('train_loss_epoch')\n",
    "        val_loss = self.trainer.callback_metrics.get('val_loss')\n",
    "\n",
    "        current_epoch = self.current_epoch\n",
    "        t_loss_str = f\"{train_loss:.4f}\" if train_loss is not None else \"N/A\"\n",
    "        v_loss_str = f\"{val_loss:.4f}\" if val_loss is not None else \"N/A\"\n",
    "        roc_str = f\"{score:.4f}\" if score is not None else \"N/A\"\n",
    "        self.print(f\"\\n(Epoch {current_epoch}) Train Loss: {t_loss_str} | Val Loss: {v_loss_str} | ROC AUC: {roc_str}\")        \n",
    "\n",
    "    def on_predict_start(self):\n",
    "        self.tta_model = tta.ClassificationTTAWrapper(\n",
    "            self.model, \n",
    "            self.tta_transforms, \n",
    "            merge_mode='mean'\n",
    "        )\n",
    "\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        if isinstance(batch, (list, tuple)):\n",
    "            x = batch[0]\n",
    "        else:\n",
    "            x = batch\n",
    "        \n",
    "        outputs = self.tta_model(x)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Training Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Metrics & Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "OdZxEQE7yhZy"
   },
   "outputs": [],
   "source": [
    "class MetricHandler:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.preds_list = []\n",
    "        self.actual_list = []\n",
    "\n",
    "    def update(self, preds, actual):\n",
    "        self.preds_list.extend(preds)\n",
    "        self.actual_list.extend(actual)\n",
    "\n",
    "    def compute_roc_auc(self):\n",
    "        return roc_auc_score(self.actual_list, self.preds_list)\n",
    "    \n",
    "    \n",
    "class BackupHandler:\n",
    "    def __init__(self, local_dir, backup_dir=None, active=True):\n",
    "        self.local_dir = local_dir\n",
    "        self.backup_dir = backup_dir\n",
    "        self.active = active and (backup_dir is not None)\n",
    "\n",
    "        if self.active and self.backup_dir is not None:\n",
    "            os.makedirs(self.backup_dir, exist_ok=True)\n",
    "            print(f'Backup Active : {self.local_dir} -> {self.backup_dir}')\n",
    "\n",
    "    def backup(self, filename):\n",
    "        if not self.active or self.backup_dir is None:\n",
    "            return\n",
    "\n",
    "        src_path = os.path.join(self.local_dir, filename)\n",
    "        dst_path = os.path.join(self.backup_dir, filename)\n",
    "        \n",
    "        if os.path.exists(src_path):\n",
    "            shutil.copy(src_path, dst_path)\n",
    "\n",
    "    def save_file(self, data, filename, logit=False):\n",
    "        local_path = os.path.join(self.local_dir, filename)\n",
    "\n",
    "        if logit:\n",
    "            np.save(local_path, data)\n",
    "            print(f'Logit saved at {local_path}')\n",
    "\n",
    "        else:\n",
    "            data.to_csv(local_path, index=False)\n",
    "            print(f'CSV saved at {local_path}')\n",
    "\n",
    "        self.backup(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Experiment Ochestrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentRunner:\n",
    "    \"\"\"\n",
    "    K-Fold 교차 검증 및 전체 실험 프로세스를 지휘하는 오케스트레이터 클래스입니다.\n",
    "    \n",
    "    환경 설정(Kaggle, Colab, Local)에 따른 경로 자동화부터 WandB 로깅, 체크포인트 저장, \n",
    "    K-Fold 학습 루프 제어 및 최종 추론(OOF 및 Test)까지의 전체 워크플로우를 담당합니다.\n",
    "    실험이 종료될 때마다 명시적인 메모리 정리(GC, CUDA Cache)를 수행하여 \n",
    "    리소스 사용을 최적화하고 연속적인 실험 안정성을 보장합니다.\n",
    "    \"\"\"\n",
    "    def __init__(self, config, train_df, teacher_logit, test_df):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.train_df = train_df\n",
    "        self.teacher_logit = teacher_logit\n",
    "        self.test_df = test_df\n",
    "        self.paths = self._setup_env()\n",
    "        self.backup_handler = BackupHandler(local_dir=self.paths.local_path , backup_dir=self.paths.drive_path, active=False)\n",
    "        \n",
    "    def _setup_env(self):\n",
    "        is_kaggle = os.path.exists('/kaggle/') \n",
    "        is_colab = os.path.exists('/content/drive/Mydrive') and not is_kaggle\n",
    "\n",
    "        if is_kaggle:\n",
    "            print(\"Environment: Kaggle\")\n",
    "            drive_path = None\n",
    "            local_path = '/kaggle/working/'\n",
    "        elif is_colab:\n",
    "            print(\"Environment: Google Colab\")\n",
    "            drive_path = f'/content/drive/MyDrive/Kaggle_Save/{CFG.exp_name}/'\n",
    "            local_path = '/content/models/'\n",
    "        else:\n",
    "            print(\"Environment: Local\")\n",
    "            drive_path = None\n",
    "            local_path = f'../data/models/{CFG.exp_name}/'\n",
    "        \n",
    "        print(f\"Save Path: {local_path}\")\n",
    "        return SimpleNamespace(local_path=local_path, drive_path=drive_path)    \n",
    "    \n",
    "    def run(self):\n",
    "        for fold in range(self.config.n_folds):\n",
    "            print('='*30, f'FOLD {fold+1}', '='*30)\n",
    "            \n",
    "            wandb_logger = WandbLogger(\n",
    "                project=self.config.project_name,\n",
    "                group=self.config.exp_name,\n",
    "                settings=wandb.Settings(program=current_file),\n",
    "                name=f\"Fold_{fold+1}\",\n",
    "                job_type=\"train\",\n",
    "                save_code=True,\n",
    "                config={k: v for k, v in self.config.__dict__.items() if not k.startswith('__')}\n",
    "            )\n",
    "            \n",
    "            train_len = len(self.train_df[self.train_df['fold'] != fold])\n",
    "            steps_per_epoch = math.ceil(train_len / self.config.batch_size / self.config.accum_iter)\n",
    "\n",
    "            datamodule = PlantDataModule(train_df=self.train_df, teacher_logit=self.teacher_logit, test_df=self.test_df, cfg=self.config, fold_idx=fold)\n",
    "            model = PlantDiseaseModule(self.config, steps_per_epoch=steps_per_epoch)\n",
    "            \n",
    "            ckpt_callback = pl.callbacks.ModelCheckpoint(\n",
    "                monitor='val_roc_auc',\n",
    "                mode='max',\n",
    "                save_top_k=self.config.top_k,\n",
    "                save_weights_only=True,\n",
    "                save_last=False,\n",
    "                dirpath=self.paths.local_path,\n",
    "                filename=f'Fold{fold+1}-Ep{{epoch:02d}}-{{val_roc_auc:.4f}}',\n",
    "                auto_insert_metric_name=False,\n",
    "            )\n",
    "            progress_bar = TQDMProgressBar(refresh_rate=1)\n",
    "            alpha_callback = FoldAlphaCallback(\n",
    "                current_fold=fold,\n",
    "                weak_folds=[0, 1],\n",
    "                weak_alpha=self.config.weak_alpha, \n",
    "                strong_alpha=self.config.strong_alpha\n",
    "            )\n",
    "            \n",
    "            trainer = pl.Trainer(\n",
    "                max_epochs=self.config.epochs,\n",
    "                accelerator='auto',\n",
    "                precision='16-mixed',\n",
    "                accumulate_grad_batches=self.config.accum_iter,\n",
    "                callbacks=[ckpt_callback, progress_bar, alpha_callback],\n",
    "                logger=wandb_logger,\n",
    "                log_every_n_steps=10\n",
    "            )\n",
    "\n",
    "            trainer.fit(model, datamodule=datamodule)\n",
    "            \n",
    "            print(f'\\n Top-{ckpt_callback.save_top_k} Models in this Fold:')\n",
    "            for path, score in ckpt_callback.best_k_models.items():\n",
    "                model_name = os.path.basename(path)\n",
    "                print(f'> {model_name}')\n",
    "                \n",
    "            wandb.finish()\n",
    "            \n",
    "            # 메모리 정리\n",
    "            del datamodule, trainer, model\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "    def _load_averaged_model(self, fold):\n",
    "        save_path = os.path.join(self.paths.local_path, f'best_score_model_{fold+1}.pth')\n",
    "        model = PlantDiseaseModule(self.config)\n",
    "\n",
    "        if os.path.exists(save_path):\n",
    "            print(f'Found existing averaged model for Fold {fold+1}. Loading directly...')\n",
    "            state_dict = torch.load(save_path, map_location=device)\n",
    "            model.load_state_dict(state_dict)\n",
    "        else:\n",
    "            print(f'Merging Top-K Models for Fold {fold+1} ...')\n",
    "            score_pattern = os.path.join(self.paths.local_path, f'Fold{fold+1}-Ep*.ckpt')\n",
    "            score_files = glob.glob(score_pattern)\n",
    "            print(f'Found {len(score_files)} score models : {[os.path.basename(f) for f in score_files]}')\n",
    "            \n",
    "            first_state = torch.load(score_files[0], map_location='cpu')['state_dict']\n",
    "            avg_state_dict = {}\n",
    "            for k, v in first_state.items():\n",
    "                if v.is_floating_point():\n",
    "                    avg_state_dict[k] = v.float() # Float32로 변환하여 초기화\n",
    "                else:\n",
    "                    avg_state_dict[k] = v \n",
    "            \n",
    "            if len(score_files) > 1:\n",
    "                for path in score_files[1:]:\n",
    "                    state_dict = torch.load(path, map_location='cpu')['state_dict']\n",
    "                    for key in avg_state_dict:\n",
    "                        if avg_state_dict[key].is_floating_point():\n",
    "                            avg_state_dict[key] += state_dict[key].float()\n",
    "                        else:\n",
    "                            pass\n",
    "                for key in avg_state_dict:\n",
    "                    if avg_state_dict[key].is_floating_point():\n",
    "                        avg_state_dict[key] = avg_state_dict[key] / len(score_files)\n",
    "            \n",
    "            model.load_state_dict(avg_state_dict)\n",
    "            \n",
    "            for remove_path in score_files:\n",
    "                if os.path.exists(remove_path):\n",
    "                    os.remove(remove_path)\n",
    "            \n",
    "            save_path = os.path.join(self.paths.local_path, f'best_score_model_{fold+1}.pth')\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print('Save Avg Model : ', save_path)\n",
    "        \n",
    "        if self.config.is_bn:\n",
    "            print('Update BN stats ... ')\n",
    "            model = model.to(self.config.device)\n",
    "            model.train()\n",
    "\n",
    "            train_subset = self.train_df[self.train_df['fold'] != fold].reset_index(drop=True)\n",
    "            transform_test = A.Compose([\n",
    "                A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "                ToTensorV2()\n",
    "            ])\n",
    "            dataset_bn = ImageDataset(train_subset, transform=transform_test, is_test=True)\n",
    "            loader_bn = DataLoader(\n",
    "                dataset_bn, \n",
    "                batch_size=self.config.batch_size, \n",
    "                shuffle=True,\n",
    "                num_workers=self.config.num_workers,\n",
    "                pin_memory=True,\n",
    "                drop_last=True\n",
    "            )\n",
    "            \n",
    "            update_bn(loader_bn, model, device=self.config.device)\n",
    "            model.eval()\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "        else:\n",
    "            print('Skipping BN update for LayerNorm')\n",
    "            model = model.to(self.config.device)\n",
    "        return model\n",
    "\n",
    "    def find_optimal_temperature(self, logits, labels):\n",
    "        \"\"\"\n",
    "        OOF Logits와 정답을 이용해 NLL을 최소화하는 T 값 탐색\n",
    "        \"\"\"\n",
    "        # 정답 라벨 처리 (One-hot -> Index)\n",
    "        if labels.ndim > 1:\n",
    "            labels = np.argmax(labels, axis=1)\n",
    "        \n",
    "        logits_tensor = torch.tensor(logits, dtype=torch.float32)\n",
    "        labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "        \n",
    "        # NLL Loss\n",
    "        t_candidates = np.arange(0.5, 2.6, 0.1)\n",
    "        best_t = min(t_candidates, key=lambda t: torch.nn.CrossEntropyLoss()(logits_tensor / t, labels_tensor).item())\n",
    "        print(f\"    > Best T: {best_t:.1f}\")\n",
    "        return best_t\n",
    "\n",
    "    # weight average\n",
    "    def run_inference(self):\n",
    "        oof_preds = np.zeros((len(self.train_df), 4))\n",
    "        oof_preds_og = np.zeros((len(self.train_df), 4))\n",
    "        oof_preds_logit = np.zeros((len(self.train_df), 4))\n",
    "\n",
    "        final_preds = np.zeros((len(self.test_df), 4))\n",
    "        final_preds_og = np.zeros((len(self.test_df), 4))\n",
    "\n",
    "        for fold in range(self.config.n_folds):\n",
    "            print(f'=== Inference Fold {fold+1} ===')\n",
    "            # 가중치 평균 모델\n",
    "            avg_model = self._load_averaged_model(fold)\n",
    "\n",
    "            # 추론용 데이터 모듈 설정\n",
    "            infer_module = PlantDataModule(train_df=self.train_df, teacher_logit=self.teacher_logit, test_df=self.test_df, cfg=self.config, fold_idx=fold, inference_mode=True)\n",
    "            infer_module.setup(stage='test')\n",
    "            \n",
    "            # trainer 생성\n",
    "            progress_bar = TQDMProgressBar(refresh_rate=1)\n",
    "            infer_trainer = pl.Trainer(\n",
    "                accelerator='auto',\n",
    "                precision='16-mixed',\n",
    "                logger=False,\n",
    "                enable_checkpointing=False,\n",
    "                callbacks=[progress_bar]\n",
    "            )\n",
    "\n",
    "            # 검증셋 인덱스 및 정답 라벨\n",
    "            valid_indices = self.train_df[self.train_df['fold'] == fold].index.values\n",
    "            valid_labels = self.train_df.iloc[valid_indices][['healthy', 'multiple_diseases', 'rust', 'scab']].values\n",
    "\n",
    "            # OOF 추론\n",
    "            oof_list = infer_trainer.predict(avg_model, dataloaders=infer_module.val_dataloader())\n",
    "            current_oof_logits = torch.cat(oof_list).cpu().numpy()\n",
    "            print(f\"Max: {current_oof_logits.max()}, Min: {current_oof_logits.min()}\")\n",
    "\n",
    "            # 최적 Calibration T 찾기\n",
    "            optimal_t = self.find_optimal_temperature(current_oof_logits, valid_labels)\n",
    "\n",
    "            # OOF 보정 및 확률 변환\n",
    "            calibrated_oof_probs = torch.softmax(torch.tensor(current_oof_logits) / optimal_t, dim=1).numpy()\n",
    "            og_oof_probs = torch.softmax(torch.tensor(current_oof_logits), dim=1).numpy()\n",
    "            oof_preds[valid_indices] = calibrated_oof_probs\n",
    "            oof_preds_og[valid_indices] = og_oof_probs\n",
    "            oof_preds_logit[valid_indices] = current_oof_logits\n",
    "\n",
    "            # Test 추론 및 보정\n",
    "            sub_list = infer_trainer.predict(avg_model, dataloaders=infer_module.predict_dataloader())\n",
    "            current_test_logits = torch.cat(sub_list).cpu().numpy()\n",
    "            calibrated_test_probs = torch.softmax(torch.tensor(current_test_logits) / optimal_t, dim=1).numpy()\n",
    "            test_probs = torch.softmax(torch.tensor(current_test_logits), dim=1).numpy()\n",
    "            final_preds += calibrated_test_probs\n",
    "            final_preds_og += test_probs\n",
    "\n",
    "            # 메모리 정리\n",
    "            del avg_model, infer_trainer, infer_module\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "        final_preds /= self.config.n_folds\n",
    "        final_preds_og /= self.config.n_folds\n",
    "\n",
    "        # 최종 메트릭 계산\n",
    "        metric_handler = MetricHandler()\n",
    "        metric_handler.update(oof_preds, self.train_df[hard_cols].values)\n",
    "        oof_roc = metric_handler.compute_roc_auc()\n",
    "        print(f'\\n>>> Final OOF ROC AUC : {oof_roc:.5f}')\n",
    "\n",
    "        return oof_preds, oof_preds_og, oof_preds_logit, final_preds, final_preds_og"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Training Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = ExperimentRunner(config=CFG, train_df=train_df, teacher_logit=teacher_logit, test_df=test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Inference & Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "oof_preds, oof_preds_og, oof_preds_logit, final_preds, final_preds_og = runner.run_inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_oof = train_df[['image_id']].copy()\n",
    "result_oof[hard_cols] = oof_preds\n",
    "runner.backup_handler.save_file(result_oof, f'oof_preds_{CFG.exp_name}.csv')\n",
    "result_oof[hard_cols] = oof_preds_og\n",
    "runner.backup_handler.save_file(result_oof, f'oof_preds_og_{CFG.exp_name}.csv')\n",
    "runner.backup_handler.save_file(oof_preds_logit, f'oof_logit_{CFG.exp_name}.npy', logit=True)\n",
    "\n",
    "result_sub = submission[['image_id']].copy()\n",
    "result_sub[hard_cols] = final_preds\n",
    "runner.backup_handler.save_file(result_sub, f'submission_{CFG.exp_name}.csv')\n",
    "result_sub[hard_cols] = final_preds_og\n",
    "runner.backup_handler.save_file(result_sub, f'submission_og_{CFG.exp_name}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N1w8ZC3_EJie",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "display(result_oof.head())\n",
    "display(result_sub.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_file1 = os.path.join(runner.backup_handler.local_dir, f'submission_{CFG.exp_name}.csv')\n",
    "sub_file2 = os.path.join(runner.backup_handler.local_dir, f'submission_og_{CFG.exp_name}.csv')\n",
    "print(sub_file1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['KAGGLE_CONFIG_DIR'] = \"/teamspace/studios/this_studio/\"\n",
    "!kaggle competitions submit -c plant-pathology-2020-fgvc7 -f {sub_file1} -m \"{CFG.exp_name}_cali\"\n",
    "!kaggle competitions submit -c plant-pathology-2020-fgvc7 -f {sub_file2} -m \"{CFG.exp_name}_og\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 1026645,
     "sourceId": 18648,
     "sourceType": "competition"
    },
    {
     "datasetId": 9113217,
     "sourceId": 14395474,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31236,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
